level=info msg="Skipped reading configuration file" reason="Config File \"ciliumd\" Not Found in \"[/root]\"" subsys=config
level=info msg="Memory available for map entries (0.003% of 8033759232B): 20084398B" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config
level=info msg="  --agent-health-port='9876'" subsys=daemon
level=info msg="  --agent-labels=''" subsys=daemon
level=info msg="  --allow-icmp-frag-needed='true'" subsys=daemon
level=info msg="  --allow-localhost='auto'" subsys=daemon
level=info msg="  --annotate-k8s-node='true'" subsys=daemon
level=info msg="  --auto-create-cilium-node-resource='true'" subsys=daemon
level=info msg="  --auto-direct-node-routes='false'" subsys=daemon
level=info msg="  --blacklist-conflicting-routes='true'" subsys=daemon
level=info msg="  --bpf-compile-debug='false'" subsys=daemon
level=info msg="  --bpf-ct-global-any-max='262144'" subsys=daemon
level=info msg="  --bpf-ct-global-tcp-max='524288'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-any='1m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-tcp='6h0m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-tcp-fin='10s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-tcp-syn='1m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-service-any='1m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-service-tcp='6h0m0s'" subsys=daemon
level=info msg="  --bpf-fragments-map-max='8192'" subsys=daemon
level=info msg="  --bpf-map-dynamic-size-ratio='0.0025'" subsys=daemon
level=info msg="  --bpf-nat-global-max='524288'" subsys=daemon
level=info msg="  --bpf-neigh-global-max='524288'" subsys=daemon
level=info msg="  --bpf-policy-map-max='16384'" subsys=daemon
level=info msg="  --bpf-root=''" subsys=daemon
level=info msg="  --bpf-sock-rev-map-max='262144'" subsys=daemon
level=info msg="  --certificates-directory='/var/run/cilium/certs'" subsys=daemon
level=info msg="  --cgroup-root=''" subsys=daemon
level=info msg="  --cluster-id='0'" subsys=daemon
level=info msg="  --cluster-name='default'" subsys=daemon
level=info msg="  --clustermesh-config='/var/lib/cilium/clustermesh/'" subsys=daemon
level=info msg="  --cmdref=''" subsys=daemon
level=info msg="  --config=''" subsys=daemon
level=info msg="  --config-dir='/tmp/cilium/config-map'" subsys=daemon
level=info msg="  --conntrack-gc-interval='0s'" subsys=daemon
level=info msg="  --datapath-mode='veth'" subsys=daemon
level=info msg="  --debug='true'" subsys=daemon
level=info msg="  --debug-verbose=''" subsys=daemon
level=info msg="  --device=''" subsys=daemon
level=info msg="  --devices=''" subsys=daemon
level=info msg="  --direct-routing-device=''" subsys=daemon
level=info msg="  --disable-cnp-status-updates='true'" subsys=daemon
level=info msg="  --disable-conntrack='false'" subsys=daemon
level=info msg="  --disable-endpoint-crd='false'" subsys=daemon
level=info msg="  --disable-envoy-version-check='false'" subsys=daemon
level=info msg="  --disable-iptables-feeder-rules=''" subsys=daemon
level=info msg="  --disable-ipv4='false'" subsys=daemon
level=info msg="  --disable-k8s-services='false'" subsys=daemon
level=info msg="  --egress-masquerade-interfaces=''" subsys=daemon
level=info msg="  --enable-auto-protect-node-port-range='true'" subsys=daemon
level=info msg="  --enable-bpf-clock-probe='true'" subsys=daemon
level=info msg="  --enable-bpf-masquerade='true'" subsys=daemon
level=info msg="  --enable-endpoint-health-checking='true'" subsys=daemon
level=info msg="  --enable-endpoint-routes='false'" subsys=daemon
level=info msg="  --enable-external-ips='true'" subsys=daemon
level=info msg="  --enable-health-checking='true'" subsys=daemon
level=info msg="  --enable-host-firewall='false'" subsys=daemon
level=info msg="  --enable-host-port='true'" subsys=daemon
level=info msg="  --enable-host-reachable-services='false'" subsys=daemon
level=info msg="  --enable-hubble='false'" subsys=daemon
level=info msg="  --enable-identity-mark='true'" subsys=daemon
level=info msg="  --enable-ip-masq-agent='false'" subsys=daemon
level=info msg="  --enable-ipsec='false'" subsys=daemon
level=info msg="  --enable-ipv4='true'" subsys=daemon
level=info msg="  --enable-ipv4-fragment-tracking='true'" subsys=daemon
level=info msg="  --enable-ipv6='false'" subsys=daemon
level=info msg="  --enable-k8s-api-discovery='false'" subsys=daemon
level=info msg="  --enable-k8s-endpoint-slice='true'" subsys=daemon
level=info msg="  --enable-k8s-event-handover='false'" subsys=daemon
level=info msg="  --enable-l7-proxy='true'" subsys=daemon
level=info msg="  --enable-local-node-route='true'" subsys=daemon
level=info msg="  --enable-node-port='true'" subsys=daemon
level=info msg="  --enable-policy='default'" subsys=daemon
level=info msg="  --enable-remote-node-identity='true'" subsys=daemon
level=info msg="  --enable-selective-regeneration='true'" subsys=daemon
level=info msg="  --enable-session-affinity='true'" subsys=daemon
level=info msg="  --enable-tracing='false'" subsys=daemon
level=info msg="  --enable-well-known-identities='false'" subsys=daemon
level=info msg="  --enable-xt-socket-fallback='true'" subsys=daemon
level=info msg="  --encrypt-interface=''" subsys=daemon
level=info msg="  --encrypt-node='false'" subsys=daemon
level=info msg="  --endpoint-interface-name-prefix='lxc+'" subsys=daemon
level=info msg="  --endpoint-queue-size='25'" subsys=daemon
level=info msg="  --endpoint-status=''" subsys=daemon
level=info msg="  --envoy-log=''" subsys=daemon
level=info msg="  --exclude-local-address=''" subsys=daemon
level=info msg="  --fixed-identity-mapping='map[]'" subsys=daemon
level=info msg="  --flannel-master-device=''" subsys=daemon
level=info msg="  --flannel-uninstall-on-exit='false'" subsys=daemon
level=info msg="  --force-local-policy-eval-at-source='true'" subsys=daemon
level=info msg="  --host-reachable-services-protos=''" subsys=daemon
level=info msg="  --http-403-msg=''" subsys=daemon
level=info msg="  --http-idle-timeout='0'" subsys=daemon
level=info msg="  --http-max-grpc-timeout='0'" subsys=daemon
level=info msg="  --http-request-timeout='3600'" subsys=daemon
level=info msg="  --http-retry-count='3'" subsys=daemon
level=info msg="  --http-retry-timeout='0'" subsys=daemon
level=info msg="  --hubble-event-queue-size='0'" subsys=daemon
level=info msg="  --hubble-flow-buffer-size='4095'" subsys=daemon
level=info msg="  --hubble-listen-address=''" subsys=daemon
level=info msg="  --hubble-metrics=''" subsys=daemon
level=info msg="  --hubble-metrics-server=''" subsys=daemon
level=info msg="  --hubble-socket-path='/var/run/cilium/hubble.sock'" subsys=daemon
level=info msg="  --identity-allocation-mode='crd'" subsys=daemon
level=info msg="  --identity-change-grace-period='5s'" subsys=daemon
level=info msg="  --install-iptables-rules='true'" subsys=daemon
level=info msg="  --ip-allocation-timeout='2m0s'" subsys=daemon
level=info msg="  --ip-masq-agent-config-path='/etc/config/ip-masq-agent'" subsys=daemon
level=info msg="  --ipam='kubernetes'" subsys=daemon
level=info msg="  --ipsec-key-file=''" subsys=daemon
level=info msg="  --iptables-lock-timeout='5s'" subsys=daemon
level=info msg="  --ipv4-cluster-cidr-mask-size='8'" subsys=daemon
level=info msg="  --ipv4-node='auto'" subsys=daemon
level=info msg="  --ipv4-pod-subnets=''" subsys=daemon
level=info msg="  --ipv4-range='auto'" subsys=daemon
level=info msg="  --ipv4-service-loopback-address='169.254.42.1'" subsys=daemon
level=info msg="  --ipv4-service-range='auto'" subsys=daemon
level=info msg="  --ipv6-cluster-alloc-cidr='f00d::/64'" subsys=daemon
level=info msg="  --ipv6-node='auto'" subsys=daemon
level=info msg="  --ipv6-pod-subnets=''" subsys=daemon
level=info msg="  --ipv6-range='auto'" subsys=daemon
level=info msg="  --ipv6-service-range='auto'" subsys=daemon
level=info msg="  --ipvlan-master-device='undefined'" subsys=daemon
level=info msg="  --k8s-api-server=''" subsys=daemon
level=info msg="  --k8s-force-json-patch='false'" subsys=daemon
level=info msg="  --k8s-heartbeat-timeout='30s'" subsys=daemon
level=info msg="  --k8s-kubeconfig-path=''" subsys=daemon
level=info msg="  --k8s-namespace='kube-system'" subsys=daemon
level=info msg="  --k8s-require-ipv4-pod-cidr='false'" subsys=daemon
level=info msg="  --k8s-require-ipv6-pod-cidr='false'" subsys=daemon
level=info msg="  --k8s-service-cache-size='128'" subsys=daemon
level=info msg="  --k8s-watcher-endpoint-selector='metadata.name!=kube-scheduler,metadata.name!=kube-controller-manager,metadata.name!=etcd-operator,metadata.name!=gcp-controller-manager'" subsys=daemon
level=info msg="  --k8s-watcher-queue-size='1024'" subsys=daemon
level=info msg="  --keep-bpf-templates='false'" subsys=daemon
level=info msg="  --keep-config='false'" subsys=daemon
level=info msg="  --kube-proxy-replacement='partial'" subsys=daemon
level=info msg="  --kvstore=''" subsys=daemon
level=info msg="  --kvstore-connectivity-timeout='2m0s'" subsys=daemon
level=info msg="  --kvstore-lease-ttl='15m0s'" subsys=daemon
level=info msg="  --kvstore-opt='map[]'" subsys=daemon
level=info msg="  --kvstore-periodic-sync='5m0s'" subsys=daemon
level=info msg="  --label-prefix-file=''" subsys=daemon
level=info msg="  --labels=''" subsys=daemon
level=info msg="  --lib-dir='/var/lib/cilium'" subsys=daemon
level=info msg="  --log-driver=''" subsys=daemon
level=info msg="  --log-opt='map[]'" subsys=daemon
level=info msg="  --log-system-load='false'" subsys=daemon
level=info msg="  --masquerade='true'" subsys=daemon
level=info msg="  --max-controller-interval='0'" subsys=daemon
level=info msg="  --metrics=''" subsys=daemon
level=info msg="  --monitor-aggregation='medium'" subsys=daemon
level=info msg="  --monitor-aggregation-flags='all'" subsys=daemon
level=info msg="  --monitor-aggregation-interval='5s'" subsys=daemon
level=info msg="  --monitor-queue-size='0'" subsys=daemon
level=info msg="  --mtu='0'" subsys=daemon
level=info msg="  --nat46-range='0:0:0:0:0:FFFF::/96'" subsys=daemon
level=info msg="  --native-routing-cidr=''" subsys=daemon
level=info msg="  --node-port-acceleration='disabled'" subsys=daemon
level=info msg="  --node-port-bind-protection='true'" subsys=daemon
level=info msg="  --node-port-mode='snat'" subsys=daemon
level=info msg="  --node-port-range=''" subsys=daemon
level=info msg="  --policy-audit-mode='false'" subsys=daemon
level=info msg="  --policy-queue-size='100'" subsys=daemon
level=info msg="  --policy-trigger-interval='1s'" subsys=daemon
level=info msg="  --pprof='false'" subsys=daemon
level=info msg="  --preallocate-bpf-maps='false'" subsys=daemon
level=info msg="  --prefilter-device='undefined'" subsys=daemon
level=info msg="  --prefilter-mode='native'" subsys=daemon
level=info msg="  --prepend-iptables-chains='true'" subsys=daemon
level=info msg="  --prometheus-serve-addr=''" subsys=daemon
level=info msg="  --proxy-connect-timeout='1'" subsys=daemon
level=info msg="  --read-cni-conf=''" subsys=daemon
level=info msg="  --restore='true'" subsys=daemon
level=info msg="  --sidecar-istio-proxy-image='cilium/istio_proxy'" subsys=daemon
level=info msg="  --single-cluster-route='false'" subsys=daemon
level=info msg="  --skip-crd-creation='false'" subsys=daemon
level=info msg="  --socket-path='/var/run/cilium/cilium.sock'" subsys=daemon
level=info msg="  --sockops-enable='false'" subsys=daemon
level=info msg="  --state-dir='/var/run/cilium'" subsys=daemon
level=info msg="  --tofqdns-dns-reject-response-code='refused'" subsys=daemon
level=info msg="  --tofqdns-enable-dns-compression='true'" subsys=daemon
level=info msg="  --tofqdns-enable-poller='false'" subsys=daemon
level=info msg="  --tofqdns-enable-poller-events='true'" subsys=daemon
level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='50'" subsys=daemon
level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'" subsys=daemon
level=info msg="  --tofqdns-min-ttl='0'" subsys=daemon
level=info msg="  --tofqdns-pre-cache=''" subsys=daemon
level=info msg="  --tofqdns-proxy-port='0'" subsys=daemon
level=info msg="  --tofqdns-proxy-response-max-delay='100ms'" subsys=daemon
level=info msg="  --trace-payloadlen='128'" subsys=daemon
level=info msg="  --tunnel='vxlan'" subsys=daemon
level=info msg="  --version='false'" subsys=daemon
level=info msg="  --write-cni-conf-when-ready=''" subsys=daemon
level=info msg="     _ _ _" subsys=daemon
level=info msg=" ___|_| |_|_ _ _____" subsys=daemon
level=info msg="|  _| | | | | |     |" subsys=daemon
level=info msg="|___|_|_|_|___|_|_|_|" subsys=daemon
level=info msg="Cilium 1.8.1 5ce2bc7b3 2020-07-02T20:04:47+02:00 go version go1.14.4 linux/amd64" subsys=daemon
level=info msg="cilium-envoy  version: 0a9743dda269a0b0039c9db3cf7e0a637caad7a9/1.13.3/Modified/RELEASE/BoringSSL" subsys=daemon
level=info msg="clang (10.0.0) and kernel (5.4.0) versions: OK!" subsys=linux-datapath
level=info msg="linking environment: OK!" subsys=linux-datapath
level=warning msg="BPF system config check: NOT OK." error="CONFIG_BPF kernel parameter is required" subsys=linux-datapath
level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf
level=info msg="Valid label prefix configuration:" subsys=labels-filter
level=info msg=" - :io.kubernetes.pod.namespace" subsys=labels-filter
level=info msg=" - :io.cilium.k8s.namespace.labels" subsys=labels-filter
level=info msg=" - :app.kubernetes.io" subsys=labels-filter
level=info msg=" - !:io.kubernetes" subsys=labels-filter
level=info msg=" - !:kubernetes.io" subsys=labels-filter
level=info msg=" - !:.*beta.kubernetes.io" subsys=labels-filter
level=info msg=" - !:k8s.io" subsys=labels-filter
level=info msg=" - !:pod-template-generation" subsys=labels-filter
level=info msg=" - !:pod-template-hash" subsys=labels-filter
level=info msg=" - !:controller-revision-hash" subsys=labels-filter
level=info msg=" - !:annotation.*" subsys=labels-filter
level=info msg=" - !:etcd_node" subsys=labels-filter
level=info msg="Using autogenerated IPv4 allocation range" subsys=node v4Prefix=10.2.0.0/16
level=info msg="Initializing daemon" subsys=daemon
level=info msg="Establishing connection to apiserver" host="https://10.11.0.1:443" subsys=k8s
level=info msg="Connected to apiserver" subsys=k8s
level=debug msg="Starting new controller" name=k8s-heartbeat subsys=controller uuid=c88e3e32-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 1.477µs" name=k8s-heartbeat subsys=controller uuid=c88e3e32-c8e9-11ea-9724-8aaf378a454f
level=info msg="Inheriting MTU from external network interface" device=eth0 ipAddr=172.18.0.2 mtu=1500 subsys=mtu
level=debug msg="getting identity cache for identity allocator manager" subsys=identity-cache
level=debug msg="creating new EventQueue" name=repository-change-queue numBufferedEvents=100 subsys=eventqueue
level=debug msg="creating new EventQueue" name=repository-reaction-queue numBufferedEvents=100 subsys=eventqueue
level=debug msg="Performing regular background work" subsys=nodemanager syncInterval=1m0s
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lxc subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipcache subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_metrics subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_tunnel_map subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_services_v2 subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_backends subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_reverse_nat subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_events subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_events subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_signals subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_signals subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_call_policy subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_call_policy subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_nodeport_neigh4 subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_nodeport_neigh4 subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipv4_frag_datagrams subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipv4_frag_datagrams subsys=bpf
level=debug msg="Starting new controller" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 17.122µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb_affinity_match subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_affinity subsys=bpf
level=info msg="Restored services from maps" failed=0 restored=0 subsys=service
level=info msg="Creating CRD (CustomResourceDefinition)..." name=CiliumNetworkPolicy/v2 subsys=k8s
level=debug msg="Checking if CRD (CustomResourceDefinition) needs update..." name=CiliumNetworkPolicy/v2 subsys=k8s
level=debug msg="Waiting for CRD (CustomResourceDefinition) to be available..." name=CiliumNetworkPolicy/v2 subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=CiliumNetworkPolicy/v2 subsys=k8s
level=info msg="Creating CRD (CustomResourceDefinition)..." name=CiliumClusterwideNetworkPolicy/v2 subsys=k8s
level=debug msg="Checking if CRD (CustomResourceDefinition) needs update..." name=CiliumClusterwideNetworkPolicy/v2 subsys=k8s
level=debug msg="Waiting for CRD (CustomResourceDefinition) to be available..." name=CiliumClusterwideNetworkPolicy/v2 subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=CiliumClusterwideNetworkPolicy/v2 subsys=k8s
level=info msg="Creating CRD (CustomResourceDefinition)..." name=v2.CiliumEndpoint subsys=k8s
level=debug msg="Checking if CRD (CustomResourceDefinition) needs update..." name=v2.CiliumEndpoint subsys=k8s
level=debug msg="Waiting for CRD (CustomResourceDefinition) to be available..." name=v2.CiliumEndpoint subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=v2.CiliumEndpoint subsys=k8s
level=info msg="Creating CRD (CustomResourceDefinition)..." name=v2.CiliumNode subsys=k8s
level=debug msg="Checking if CRD (CustomResourceDefinition) needs update..." name=v2.CiliumNode subsys=k8s
level=debug msg="Waiting for CRD (CustomResourceDefinition) to be available..." name=v2.CiliumNode subsys=k8s
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=v2.CiliumNode subsys=k8s
level=info msg="Creating CRD (CustomResourceDefinition)..." name=v2.CiliumIdentity subsys=k8s
level=debug msg="Checking if CRD (CustomResourceDefinition) needs update..." name=v2.CiliumIdentity subsys=k8s
level=debug msg="Waiting for CRD (CustomResourceDefinition) to be available..." name=v2.CiliumIdentity subsys=k8s
level=debug msg="Controller func execution time: 17.263µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=info msg="CRD (CustomResourceDefinition) is installed and up-to-date" name=v2.CiliumIdentity subsys=k8s
level=debug msg="Missing io.cilium.network.ipv4-cilium-host. Annotation required when IPSec Enabled" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Missing io.cilium.network.ipv6-cilium-host. Annotation required when IPSec Enabled" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Empty IPv6 CIDR annotation in node" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Empty IPv4 health endpoint annotation in node" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Empty IPv6 health endpoint annotation in node" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=info msg="Retrieved node information from kubernetes node" nodeName=kind-worker subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.18.0.2 ipAddr.ipv6="<nil>" k8sNodeIP=172.18.0.2 nodeName=kind-worker subsys=k8s v4Prefix=10.10.1.0/24 v6Prefix="<nil>"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=debug msg="Found default route on node {Ifindex: 60 Dst: <nil> Src: <nil> Gw: 172.18.0.1 Flags: [] Table: 254}" subsys=route
level=info msg="Using auto-derived devices for BPF node port" devices="[eth0]" directRoutingDevice=eth0 subsys=daemon
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="networking.k8s.io/v1::NetworkPolicy" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="discovery/v1beta1::EndpointSlice" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Service" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumNetworkPolicy" subsys=k8s-watcher
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints= k8sNamespace=kube-system k8sSvcName=kube-dns old-service=nil service="frontend:10.11.0.10/ports=[dns dns-tcp metrics]/selector=map[k8s-app:kube-dns]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[]" serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Resolving service" l3n4Addr="{IP:10.11.0.10 L4Addr:{Protocol:TCP Port:9153} Scope:0}" subsys=service
level=debug msg="Acquired service ID" backends="[]" serviceID=1 serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=1 subsys=service
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.10:9153" svcVal="0 (1) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=1 subsys=service
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumClusterwideNetworkPolicy" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[]" serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Resolving service" l3n4Addr="{IP:10.11.0.10 L4Addr:{Protocol:UDP Port:53} Scope:0}" subsys=service
level=debug msg="Acquired service ID" backends="[]" serviceID=2 serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.10:53" svcVal="0 (2) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=2 subsys=service
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints="172.18.0.3:6443/TCP" k8sNamespace=default k8sSvcName=kubernetes old-service=nil service="frontend:10.11.0.1/ports=[https]/selector=map[]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[{0  {172.18.0.3 {TCP 6443} 0}}]" serviceIP="{10.11.0.1 {TCP 443} 0}" serviceName=kubernetes serviceNamespace=default sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Resolving service" l3n4Addr="{IP:10.11.0.1 L4Addr:{Protocol:TCP Port:443} Scope:0}" subsys=service
level=debug msg="Acquired service ID" backends="[{0  {172.18.0.3 {TCP 6443} 0}}]" serviceID=3 serviceIP="{10.11.0.1 {TCP 443} 0}" serviceName=kubernetes serviceNamespace=default sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=3 subsys=service
level=debug msg="Adding new backend" backendID=1 backends="[{0  {172.18.0.3 {TCP 6443} 0}}]" l3n4Addr="{172.18.0.3 {TCP 6443} 0}" serviceID=3 serviceIP="{10.11.0.1 {TCP 443} 0}" serviceName=kubernetes serviceNamespace=default sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserting service entry" slaveSlot=1 subsys=map-lb svcKey="10.11.0.1:443" svcVal="1 (3) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.1:443" svcVal="0 (3) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=3 subsys=service
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumNode" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Namespace" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumEndpoint" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Node" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Waiting until all pre-existing resources related to policy have been received" subsys=k8s-watcher
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=10.10.1.0/24 v6Prefix="<nil>"
level=info msg="Restoring endpoints..." subsys=daemon
level=info msg="Envoy: Starting xDS gRPC server listening on /var/run/cilium/xds.sock" subsys=envoy-manager
level=info msg="No old endpoints found." subsys=daemon
level=debug msg="Allocated random IP" ip=10.10.1.228 owner=router subsys=ipam
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: default" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: kind-worker" subsys=daemon
level=info msg="  Node-IPv6: <nil>" subsys=daemon
level=info msg="  External-Node IPv4: 172.18.0.2" subsys=daemon
level=info msg="  Internal-Node IPv4: 10.10.1.228" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.10.1.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 172.18.0.2" subsys=daemon
level=debug msg="Allocated random IP" ip=10.10.1.241 owner=health subsys=ipam
level=debug msg="IPv4 health endpoint address: 10.10.1.241" subsys=daemon
level=info msg="Annotating k8s node" subsys=daemon v4CiliumHostIP.IPv4=10.10.1.228 v4Prefix=10.10.1.0/24 v4healthIP.IPv4=10.10.1.241 v6CiliumHostIP.IPv6="<nil>" v6Prefix="<nil>" v6healthIP.IPv6="<nil>"
level=debug msg="Updating node annotations with node CIDRs" key=0 nodeName=kind-worker subsys=k8s v4CiliumHostIP.IPv4=10.10.1.228 v4Prefix=10.10.1.0/24 v4healthIP.IPv4=10.10.1.241 v6CiliumHostIP.IPv6="<nil>" v6Prefix="<nil>" v6healthIP.IPv6="<nil>"
level=debug msg="Starting new controller" name=update-k8s-node-annotations subsys=controller uuid=cbc75980-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Received node update event from local: types.Node{Name:\"kind-worker\", Cluster:\"default\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x12, 0x0, 0x2}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0xa, 0xa, 0x1, 0xe4}}}, IPv4AllocCIDR:(*cidr.CIDR)(0xc0005a82d0), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv4HealthIP:net.IP{0xa, 0xa, 0x1, 0xf1}, IPv6HealthIP:net.IP(nil), ClusterID:0, Source:\"local\", EncryptionKey:0x0, Labels:map[string]string(nil)}" subsys=nodemanager
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=172.18.0.2 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{172.18.0.2 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=10.10.1.228 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{10.10.1.228 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{health local false}" ipAddr=10.10.1.241 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=health ipAddr="{10.10.1.241 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=info msg="Adding local node to cluster" node="{kind-worker default [{InternalIP 172.18.0.2} {CiliumInternalIP 10.10.1.228}] 10.10.1.0/24 <nil> 10.10.1.241 <nil> 0 local 0 map[]}" subsys=nodediscovery
level=debug msg="Skipped ipcache map update on pod add" error="pod is using host networking" hostIP=172.18.0.2 k8sNamespace=kube-system k8sPodName=kube-proxy-ftjhw podIP=172.18.0.2 podIPs="[{172.18.0.2}]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="pod is using host networking" hostIP=172.18.0.2 k8sNamespace=kube-system k8sPodName=cilium-2qj4d podIP=172.18.0.2 podIPs="[{172.18.0.2}]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="empty PodIPs" hostIP= k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-67795f75bd-lptkp podIP= podIPs="[]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="pod is using host networking" hostIP=172.18.0.2 k8sNamespace=kube-system k8sPodName=cilium-operator-657978fb5b-9km5j podIP=172.18.0.2 podIPs="[{172.18.0.2}]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="pod is using host networking" hostIP=172.18.0.2 k8sNamespace=kube-system k8sPodName=cilium-node-init-slh5d podIP=172.18.0.2 podIPs="[{172.18.0.2}]" subsys=k8s-watcher
level=debug msg="Controller func execution time: 17.097038ms" name=update-k8s-node-annotations subsys=controller uuid=cbc75980-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=update-k8s-node-annotations subsys=controller uuid=cbc75980-c8e9-11ea-9724-8aaf378a454f
level=debug msg="cache synced" kubernetesResource="networking.k8s.io/v1::NetworkPolicy" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="core/v1::Service" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Service" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="discovery/v1beta1::EndpointSlice" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="discovery/v1beta1::EndpointSlice" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="core/v1::Namespace" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="core/v1::Node" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumNetworkPolicy" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumNetworkPolicy" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumEndpoint" subsys=k8s-watcher
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumNode" subsys=k8s-watcher
level=debug msg="Add NodeCiliumInternalIP: 10.10.1.228" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Missing io.cilium.network.ipv6-cilium-host. Annotation required when IPSec Enabled" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Empty IPv6 CIDR annotation in node" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=debug msg="Empty IPv6 health endpoint annotation in node" k8sNodeID=89c4e49b-b42c-4779-94ea-80e027a200a9 nodeName=kind-worker subsys=k8s
level=info msg="Successfully created CiliumNode resource" subsys=nodediscovery
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Cluster-ID is not specified, skipping ClusterMesh initialization" subsys=daemon
level=debug msg="Identity allocation backed by CRD" subsys=identity-cache
level=debug msg="Detach BPF Object:" args="[cgroup detach /var/run/cilium/cgroupv2 sock_ops pinned /sys/fs/bpf/bpf_sockops]" bpftool=bpftool subsys=sockops
level=debug msg="Starting new controller" name=template-dir-watcher subsys=controller uuid=cc01faa2-c8e9-11ea-9724-8aaf378a454f
level=debug msg="writing configuration" file-path=netdev_config.h subsys=datapath-loader
level=info msg="Setting up base BPF datapath (BPF v2 instruction set, ktime clock source)" subsys=datapath-loader
level=info msg="Setting sysctl net.core.bpf_jit_enable=1" subsys=datapath-loader
level=warning msg="Failed to sysctl -w" error="could not open the sysctl file /proc/sys/net/core/bpf_jit_enable: open /proc/sys/net/core/bpf_jit_enable: no such file or directory" subsys=datapath-loader sysParamName=net.core.bpf_jit_enable sysParamValue=1
level=info msg="Setting sysctl net.ipv4.conf.all.rp_filter=0" subsys=datapath-loader
level=info msg="Setting sysctl kernel.unprivileged_bpf_disabled=1" subsys=datapath-loader
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumClusterwideNetworkPolicy" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumClusterwideNetworkPolicy" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumNode" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="networking.k8s.io/v1::NetworkPolicy" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Namespace" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumEndpoint" subsys=k8s-watcher
level=info msg="All pre-existing resources related to policy have been received; continuing" subsys=k8s-watcher
level=debug msg="Initial list of identities received" subsys=allocator
level=debug msg="Controller func execution time: 18.829µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Checking local routes for conflicts..." subsys=ipam
level=debug msg="Ignoring route: no destination address" route="{Ifindex: 60 Dst: <nil> Src: <nil> Gw: 172.18.0.1 Flags: [] Table: 254}" subsys=ipam
level=info msg="Blacklisting local route as no-alloc" route=172.18.0.0/16 subsys=ipam
level=debug msg="Considering removing iptables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P FORWARD ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-KUBELET-CANARY" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A INPUT -j KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A OUTPUT -j KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment \"block incoming localnet connections\" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P PREROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P POSTROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N DOCKER_OUTPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N DOCKER_POSTROUTING" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-KUBELET-CANARY" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-LOAD-BALANCER" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-MARK-DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-MARK-MASQ" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-NODE-PORT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-POSTROUTING" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-SERVICES" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A PREROUTING -d 172.18.0.1/32 -j DOCKER_OUTPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A OUTPUT -d 172.18.0.1/32 -j DOCKER_OUTPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A POSTROUTING -d 172.18.0.1/32 -j DOCKER_POSTROUTING" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A DOCKER_OUTPUT -d 172.18.0.1/32 -p tcp -m tcp --dport 53 -j DNAT --to-destination 127.0.0.11:46495" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A DOCKER_OUTPUT -d 172.18.0.1/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:47933" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A DOCKER_POSTROUTING -s 127.0.0.11/32 -p tcp -m tcp --sport 46495 -j SNAT --to-source 172.18.0.1:53" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A DOCKER_POSTROUTING -s 127.0.0.11/32 -p udp -m udp --sport 47933 -j SNAT --to-source 172.18.0.1:53" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL -j KUBE-MARK-DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE --random-fully" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-SERVICES ! -s 10.10.0.0/16 -m comment --comment \"Kubernetes service cluster ip + port for masquerade purpose\" -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P PREROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P FORWARD ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P POSTROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-KUBELET-CANARY" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P PREROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P FORWARD ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N CILIUM_TRANSIENT_FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-KUBELET-CANARY" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A INPUT -j KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A FORWARD -m comment --comment \"cilium-feeder: CILIUM_TRANSIENT_FORWARD\" -j CILIUM_TRANSIENT_FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A OUTPUT -j KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -o cilium_host -m comment --comment \"cilium (transient): any->cluster on cilium_host forward accept\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -i cilium_host -m comment --comment \"cilium (transient): cluster->any on cilium_host forward accept (nodeport)\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -i lxc+ -m comment --comment \"cilium (transient): cluster->any on lxc+ forward accept\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -i cilium_net -m comment --comment \"cilium (transient): cluster->any on cilium_net forward accept (nodeport)\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment \"block incoming localnet connections\" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P PREROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P FORWARD ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P POSTROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P PREROUTING ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P FORWARD ACCEPT" subsys=iptables
level=debug msg="Considering removing ip6tables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P INPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P FORWARD ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-P OUTPUT ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N CILIUM_FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N CILIUM_INPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N CILIUM_OUTPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N CILIUM_TRANSIENT_FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-N KUBE-KUBELET-CANARY" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A INPUT -m comment --comment \"cilium-feeder: CILIUM_INPUT\" -j CILIUM_INPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A INPUT -j KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A FORWARD -m comment --comment \"cilium-feeder: CILIUM_FORWARD\" -j CILIUM_FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A FORWARD -m comment --comment \"cilium-feeder: CILIUM_TRANSIENT_FORWARD\" -j CILIUM_TRANSIENT_FORWARD" subsys=iptables
level=debug msg="Removing iptables rule" obj="[-w 5 -t filter -D FORWARD -m comment --comment cilium-feeder: CILIUM_TRANSIENT_FORWARD -j CILIUM_TRANSIENT_FORWARD]" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A OUTPUT -m comment --comment \"cilium-feeder: CILIUM_OUTPUT\" -j CILIUM_OUTPUT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A OUTPUT -j KUBE-FIREWALL" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_FORWARD -o cilium_host -m comment --comment \"cilium: any->cluster on cilium_host forward accept\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_FORWARD -i cilium_host -m comment --comment \"cilium: cluster->any on cilium_host forward accept (nodeport)\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_FORWARD -i lxc+ -m comment --comment \"cilium: cluster->any on lxc+ forward accept\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_FORWARD -i cilium_net -m comment --comment \"cilium: cluster->any on cilium_net forward accept (nodeport)\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_INPUT -m mark --mark 0x200/0xf00 -m comment --comment \"cilium: ACCEPT for proxy traffic\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_OUTPUT -m mark --mark 0xa00/0xfffffeff -m comment --comment \"cilium: ACCEPT for proxy return traffic\" -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_OUTPUT -m mark ! --mark 0xe00/0xf00 -m mark ! --mark 0xd00/0xf00 -m mark ! --mark 0xa00/0xe00 -m comment --comment \"cilium: host->any mark as from host\" -j MARK --set-xmark 0xc00/0xf00" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -o cilium_host -m comment --comment \"cilium (transient): any->cluster on cilium_host forward accept\" -j ACCEPT" subsys=iptables
level=debug msg="Removing iptables rule" obj="[-w 5 -t filter -D CILIUM_TRANSIENT_FORWARD -o cilium_host -m comment --comment cilium (transient): any->cluster on cilium_host forward accept -j ACCEPT]" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -i cilium_host -m comment --comment \"cilium (transient): cluster->any on cilium_host forward accept (nodeport)\" -j ACCEPT" subsys=iptables
level=debug msg="Removing iptables rule" obj="[-w 5 -t filter -D CILIUM_TRANSIENT_FORWARD -i cilium_host -m comment --comment cilium (transient): cluster->any on cilium_host forward accept (nodeport) -j ACCEPT]" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -i lxc+ -m comment --comment \"cilium (transient): cluster->any on lxc+ forward accept\" -j ACCEPT" subsys=iptables
level=debug msg="Removing iptables rule" obj="[-w 5 -t filter -D CILIUM_TRANSIENT_FORWARD -i lxc+ -m comment --comment cilium (transient): cluster->any on lxc+ forward accept -j ACCEPT]" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A CILIUM_TRANSIENT_FORWARD -i cilium_net -m comment --comment \"cilium (transient): cluster->any on cilium_net forward accept (nodeport)\" -j ACCEPT" subsys=iptables
level=debug msg="Removing iptables rule" obj="[-w 5 -t filter -D CILIUM_TRANSIENT_FORWARD -i cilium_net -m comment --comment cilium (transient): cluster->any on cilium_net forward accept (nodeport) -j ACCEPT]" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment \"block incoming localnet connections\" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod source rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT" subsys=iptables
level=debug msg="Considering removing iptables rule" obj="-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack pod destination rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT" subsys=iptables
level=info msg="Serving cilium node monitor v1.2 API at unix:///var/run/cilium/monitor1_2.sock" subsys=monitor-agent
level=debug msg="Added local ip to endpoint map" ipAddr=172.18.0.2 subsys=daemon
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=172.18.0.2 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{172.18.0.2 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Added local ip to endpoint map" ipAddr=10.10.1.228 subsys=daemon
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=10.10.1.228 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{10.10.1.228 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{world local false}" ipAddr=0.0.0.0/0 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=world ipAddr="{0.0.0.0 00000000}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Starting new controller" name=sync-endpoints-and-host-ips subsys=controller uuid=cef0da18-c8e9-11ea-9724-8aaf378a454f
level=info msg="Starting IP identity watcher" subsys=ipcache
level=debug msg="Starting new controller" name=dns-garbage-collector-job subsys=controller uuid=cef0e57d-c8e9-11ea-9724-8aaf378a454f
level=debug msg="DNS Proxy port is configured to 0. A random port will be assigned by the OS." subsys=fqdn/dnsproxy
level=debug msg="Controller func execution time: 6.987µs" name=dns-garbage-collector-job subsys=controller uuid=cef0e57d-c8e9-11ea-9724-8aaf378a454f
level=debug msg="DNS Proxy bound to address" address="[::]:39377" subsys=fqdn/dnsproxy
level=debug msg="Considering updating proxy port rules for cilium-dns-egress:39377 (old: 0)" proxy port name=cilium-dns-egress subsys=proxy
level=info msg="Adding new proxy port rules for cilium-dns-egress:39377" proxy port name=cilium-dns-egress subsys=proxy
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=172.18.0.2 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{172.18.0.2 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=10.10.1.228 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{10.10.1.228 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{world local false}" ipAddr=0.0.0.0/0 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=world ipAddr="{0.0.0.0 00000000}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 2.820667ms" name=sync-endpoints-and-host-ips subsys=controller uuid=cef0da18-c8e9-11ea-9724-8aaf378a454f
level=info msg="Validating configured node address ranges" subsys=daemon
level=info msg="Starting connection tracking garbage collector" subsys=daemon
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=info msg="Initial scan of connection tracking completed" subsys=ct-gc
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Regenerating restored endpoints" numRestored=0 subsys=daemon
level=info msg="Datapath signal listener running" subsys=signal
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Couldn't open CT map for upgrade" error="Unable to get object /sys/fs/bpf/tc/globals/cilium_ct6_global: no such file or directory" file-path=/sys/fs/bpf/tc/globals/cilium_ct6_global subsys=map-ct
level=debug msg="Couldn't open CT map for upgrade" error="Unable to get object /sys/fs/bpf/tc/globals/cilium_ct_any6_global: no such file or directory" file-path=/sys/fs/bpf/tc/globals/cilium_ct_any6_global subsys=map-ct
level=info msg="Creating host endpoint" subsys=daemon
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=endpoint-3398-regeneration-recovery subsys=controller uuid=cef26fa0-c8e9-11ea-9724-8aaf378a454f
level=debug msg="creating new EventQueue" name=endpoint-3398 numBufferedEvents=25 subsys=eventqueue
level=info msg="Finished regenerating restored endpoints" regenerated=0 subsys=daemon total=0
level=debug msg="Starting new controller" name=sync-lb-maps-with-k8s-services subsys=controller uuid=cef273f0-c8e9-11ea-9724-8aaf378a454f
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 1.227µs" name=sync-lb-maps-with-k8s-services subsys=controller uuid=cef273f0-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=sync-lb-maps-with-k8s-services subsys=controller uuid=cef273f0-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Refreshing labels of endpoint" containerID= endpointID=3398 identityLabels="reserved:host" infoLabels= subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 ipv4= ipv6= k8sPodName=/ obj="{Key:host Value: Source:reserved}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 182.079µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="reserved:host" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=host identityLabels="reserved:host" isNew=false subsys=identity-cache
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=1 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 endpointState=ready ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-3398 subsys=controller uuid=cef2951a-c8e9-11ea-9724-8aaf378a454f
level=info msg="Launching Cilium health daemon" subsys=daemon
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ reason="updated security labels" startTime="2020-07-18 11:28:31.316825462 +0000 UTC m=+12.108222782" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 endpointState=regenerating identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_03398 subsys=bpf
level=debug msg="flushing old PolicyMap" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="deleting all entries in map" file-path=/sys/fs/bpf/tc/globals/cilium_policy_03398 name=cilium_policy_03398 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 forcedRegeneration=true identity=1 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 15369 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 745 0}"
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 file-path=3398_next/ep_config.h identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 70.05µs" name=resolve-identity-3398 subsys=controller uuid=cef2951a-c8e9-11ea-9724-8aaf378a454f
level=info msg="Launching Cilium health endpoint" subsys=daemon
level=debug msg="Starting new controller" name=cilium-health-ep subsys=controller uuid=cef6939a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Compiling datapath" clang="clang version 10.0.0 (https://github.com/llvm/llvm-project.git cd5cebef40fa85627867a647d6d9632211761a82)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /usr/bin\n" debug=false llc="LLVM (http://llvm.org/):\n  LLVM version 10.0.0\n  Optimized build.\n  Default target: x86_64-unknown-linux-gnu\n  Host CPU: broadwell\n\n  Registered Targets:\n    bpf   - BPF (host endian)\n    bpfeb - BPF (big endian)\n    bpfel - BPF (little endian)\n" subsys=datapath-loader
level=debug msg="Launching compiler" args="[-emit-llvm -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=8 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/d570b2a3e940af53c376dab6c0be066b3c87c4e3 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Cannot establish connection to local cilium instance" error="Get \"http://%2Fvar%2Frun%2Fcilium%2Fcilium.sock/v1/healthz\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory" subsys=cilium-health-launcher
level=info msg="Started healthz status API server on address localhost:9876" subsys=daemon
level=debug msg="Killing old health endpoint process" pidfile=/var/run/cilium/state/health-endpoint.pid subsys=cilium-health-launcher
level=debug msg="Didn't find existing cilium-health endpoint to delete" subsys=daemon
level=debug msg="Didn't find existing device" error="Link not found" subsys=cilium-health-launcher veth=cilium_health
level=debug msg="Didn't find existing device" error="Link not found" subsys=cilium-health-launcher veth=lxc_health
level=debug msg="Unable to remove cilium-health namespace" error="Unable to delete named netns cilium-health: Cannot remove namespace file \"/var/run/netns/cilium-health\": No such file or directory\n exit status 1" subsys=cilium-health-launcher
level=debug msg="Created veth pair" subsys=endpoint-connector vethPair="[cilium lxc_health]"
level=debug msg="Spawning health endpoint with command \"ip\" [\"netns\" \"exec\" \"cilium-health\" \"cilium-health-responder\" \"--pidfile\" \"/var/run/cilium/state/health-endpoint.pid\"]" subsys=cilium-health-launcher
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Initializing Cilium API" subsys=daemon
level=info msg="Daemon initialization completed" bootstrapTime=12.253961368s subsys=daemon
level=debug msg="Setting NetworkUnavailable=false" nodeName=kind-worker subsys=k8s
level=debug msg="Starting new controller" name=mark-k8s-node-as-available subsys=controller uuid=cf13c359-c8e9-11ea-9724-8aaf378a454f
level=info msg="Hubble server is disabled" subsys=hubble
level=info msg="Serving cilium at unix:///var/run/cilium/cilium.sock" subsys=daemon
level=debug msg="Controller func execution time: 11.618683ms" name=mark-k8s-node-as-available subsys=controller uuid=cf13c359-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=mark-k8s-node-as-available subsys=controller uuid=cf13c359-c8e9-11ea-9724-8aaf378a454f
level=debug msg="GET /config request" params="{HTTPRequest:0xc0010ea700}" subsys=daemon
level=debug msg="GET /config request" params="{HTTPRequest:0xc0010eb300}" subsys=daemon
level=debug msg="Allocated random IP" ip=10.10.1.185 owner=local-path-storage/local-path-provisioner-67795f75bd-lptkp subsys=ipam
level=debug msg="PUT /endpoint/{id} request" endpoint="{Addressing:0xc0011ee380 ContainerID:8f298afa1eaa58c05d050417d3dfff589f32587214a10d122a6cab982a867cc9 ContainerName: DatapathConfiguration:<nil> DatapathMapID:0 DockerEndpointID: DockerNetworkID: HostMac:2a:db:d6:29:90:e8 ID:0 InterfaceIndex:9 InterfaceName:lxc76227b105fbe K8sNamespace:local-path-storage K8sPodName:local-path-provisioner-67795f75bd-lptkp Labels:[] Mac:02:f2:04:8e:b4:79 Pid:0 PolicyEnabled:false State:waiting-for-identity SyncBuildEndpoint:true}" subsys=daemon
level=info msg="Create endpoint request" addressing="&{10.10.1.185 cf22d4fc-c8e9-11ea-9724-8aaf378a454f  }" containerID=8f298afa1eaa58c05d050417d3dfff589f32587214a10d122a6cab982a867cc9 datapathConfiguration="<nil>" interface=lxc76227b105fbe k8sPodName=local-path-storage/local-path-provisioner-67795f75bd-lptkp labels="[]" subsys=daemon sync-build=true
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="New create request" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Namespace" subsys=k8s-watcher
level=debug msg="Connecting to k8s local stores to retrieve labels for pod" k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-67795f75bd-lptkp subsys=k8s
level=debug msg="No sidecar.istio.io/status annotation" k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-67795f75bd-lptkp subsys=k8s
level=debug msg="Starting new controller" name=endpoint-1909-regeneration-recovery subsys=controller uuid=cf23a204-c8e9-11ea-9724-8aaf378a454f
level=debug msg="creating new EventQueue" name=endpoint-1909 numBufferedEvents=25 subsys=eventqueue
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="48.54µs" eventEnqueueWaitTime=663ns eventHandlingDuration="125.64µs" eventType="*endpoint.EndpointPolicyVisibilityEvent" name=endpoint-1909 subsys=eventqueue
level=debug msg="Refreshing labels of endpoint" containerID=8f298afa1e endpointID=1909 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" infoLabels="k8s:pod-template-hash=67795f75bd" subsys=endpoint
level=debug msg="Assigning information label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ obj="{Key:pod-template-hash Value:67795f75bd Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ obj="{Key:io.cilium.k8s.policy.serviceaccount Value:local-path-provisioner-service-account Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ obj="{Key:io.cilium.k8s.policy.cluster Value:default Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ obj="{Key:app Value:local-path-provisioner Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ obj="{Key:io.kubernetes.pod.namespace Value:local-path-storage Source:k8s}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" subsys=identity-cache
level=debug msg="Allocating key" key="[k8s:app=local-path-provisioner k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account k8s:io.kubernetes.pod.namespace=local-path-storage]" subsys=allocator
level=debug msg="Allocating new master ID" key="k8s:app=local-path-provisioner;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account;k8s:io.kubernetes.pod.namespace=local-path-storage;" subsys=allocator
level=info msg="Skipped non-kubernetes labels when labelling ciliumidentity. All labels will still be used in identity determination" labels="map[]" subsys=crd-allocator
level=debug msg="Getting CEP during an initialization" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="UpdateIdentities: Adding a new identity" identity=26160 labels="[k8s:app=local-path-provisioner k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account k8s:io.kubernetes.pod.namespace=local-path-storage]" subsys=policy
level=debug msg="Regenerating all endpoints" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=info msg="Invalid state transition skipped" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 endpointState.from=waiting-for-identity endpointState.to=waiting-to-regenerate file=/go/src/github.com/cilium/cilium/pkg/endpoint/policy.go ipv4= ipv6= k8sPodName=/ line=476 subsys=endpoint
level=debug msg="Skipped invalid state transition to waiting-to-regenerate due to: Triggering endpoint regeneration due to one or more identities created or deleted" code=Warning containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Allocated new global key" key="k8s:app=local-path-provisioner;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account;k8s:io.kubernetes.pod.namespace=local-path-storage;" subsys=allocator
level=debug msg="Allocated key" id=26160 key="[k8s:app=local-path-provisioner k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account k8s:io.kubernetes.pod.namespace=local-path-storage]" subsys=allocator
level=debug msg="Resolved identity" identity=26160 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" isNew=true isNewLocally=true subsys=identity-cache
level=debug msg="UpdateIdentities: Skipping add of an existing identical identity" identity=26160 subsys=policy
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=26160 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 endpointState=ready ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" ipv4= ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 endpointState=waiting-to-regenerate identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-1909 subsys=controller uuid=cf2543aa-c8e9-11ea-9724-8aaf378a454f
level=info msg="Waiting for endpoint to be generated" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 identityLabels="k8s:app=local-path-provisioner,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account,k8s:io.kubernetes.pod.namespace=local-path-storage" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 64.006µs" name=resolve-identity-1909 subsys=controller uuid=cf2543aa-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ reason="updated security labels" startTime="2020-07-18 11:28:31.648426514 +0000 UTC m=+12.439823826" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 endpointState=regenerating identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_01909 subsys=bpf
level=debug msg="flushing old PolicyMap" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="deleting all entries in map" file-path=/sys/fs/bpf/tc/globals/cilium_policy_01909 name=cilium_policy_01909 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 forcedRegeneration=true identity=26160 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 16244 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 906 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=2 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="inserting resource into cache" subsys=xds xdsCachedVersion=2 xdsResourceName=10.10.1.185 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="committing cache transaction and notifying of new version" subsys=xds xdsCachedVersion=2 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 file-path=1909_next/ep_config.h identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Upserting IP into ipcache layer" identity="{unmanaged custom-resource false}" ipAddr=10.10.1.185 k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-67795f75bd-lptkp key=0 namedPorts="map[]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=unmanaged ipAddr="{10.10.1.185 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 38.374197ms" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Compiling datapath" clang="clang version 10.0.0 (https://github.com/llvm/llvm-project.git cd5cebef40fa85627867a647d6d9632211761a82)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /usr/bin\n" debug=false llc="LLVM (http://llvm.org/):\n  LLVM version 10.0.0\n  Optimized build.\n  Default target: x86_64-unknown-linux-gnu\n  Host CPU: broadwell\n\n  Registered Targets:\n    bpf   - BPF (host endian)\n    bpfeb - BPF (big endian)\n    bpfel - BPF (little endian)\n" subsys=datapath-loader
level=debug msg="Launching compiler" args="[-emit-llvm -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=8 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Sending request for /cluster/nodes ..." subsys=health-server
level=debug msg="Got cilium /cluster/nodes" subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probe successful" ipAddr=172.18.0.2 nodeName=kind-worker rtt="929.314µs" subsys=health-server
level=debug msg="cilium-health agent running" pidfile=/var/run/cilium/state/health-endpoint.pid subsys=cilium-health-launcher
level=debug msg="Adding route" command="ip route add 10.10.1.228/32 dev cilium" netns=cilium-health subsys=cilium-health-launcher
level=debug msg="Adding route" command="ip route add 0.0.0.0/0 via 10.10.1.228 mtu 1450 dev cilium" netns=cilium-health subsys=cilium-health-launcher
level=debug msg="Running \"ip [netns exec cilium-health bash -c ip route add 10.10.1.228/32 dev cilium && ip route add 0.0.0.0/0 via 10.10.1.228 mtu 1450 dev cilium]\"" subsys=cilium-health-launcher
level=debug msg="Starting new controller" name=endpoint-243-regeneration-recovery subsys=controller uuid=cf9a8973-c8e9-11ea-9724-8aaf378a454f
level=debug msg="creating new EventQueue" name=endpoint-243 numBufferedEvents=25 subsys=eventqueue
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Refreshing labels of endpoint" containerID= endpointID=243 identityLabels="reserved:health" infoLabels= subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 ipv4= ipv6= k8sPodName=/ obj="{Key:health Value: Source:reserved}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identityLabels="reserved:health" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identityLabels="reserved:health" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="reserved:health" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=health identityLabels="reserved:health" isNew=false subsys=identity-cache
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 identityLabels="reserved:health" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=4 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 endpointState=ready ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 identityLabels="reserved:health" ipv4= ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 endpointState=waiting-to-regenerate identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-243 subsys=controller uuid=cf9ac638-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 1.075957083s" name=cilium-health-ep subsys=controller uuid=cef6939a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 1.320277ms" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ reason="updated security labels" startTime="2020-07-18 11:28:32.418954042 +0000 UTC m=+13.210351370" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 endpointState=regenerating identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 identityLabels="reserved:health" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 779.934µs" name=resolve-identity-243 subsys=controller uuid=cf9ac638-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_00243 subsys=bpf
level=debug msg="flushing old PolicyMap" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="deleting all entries in map" file-path=/sys/fs/bpf/tc/globals/cilium_policy_00243 name=cilium_policy_00243 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 forcedRegeneration=true identity=4 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 20571 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 949 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=3 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="inserting resource into cache" subsys=xds xdsCachedVersion=3 xdsResourceName=10.10.1.241 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="committing cache transaction and notifying of new version" subsys=xds xdsCachedVersion=3 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 file-path=243_next/ep_config.h identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Regenerating all endpoints" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 endpointState=waiting-to-regenerate identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 endpointState=waiting-to-regenerate identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Skipped duplicate endpoint regeneration level no-rebuild trigger due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Greeting host" host="http://172.18.0.2:4240" ipAddr=172.18.0.2 nodeName=kind-worker path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://172.18.0.2:4240" ipAddr=172.18.0.2 nodeName=kind-worker path="Via L3" rtt="579.408µs" subsys=health-server
level=debug msg="Greeting host" host="http://10.10.1.241:4240" ipAddr=10.10.1.241 nodeName=kind-worker path="Via L3" subsys=health-server
level=info msg="Compiled new BPF template" BPFCompilationTime=2.547552033s file-path=/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de/bpf_lxc.o subsys=datapath-loader
level=debug msg="Watching template path" file-path=/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de/bpf_lxc.o subsys=datapath-loader
level=debug msg="Found variable with offset 128588" subsys=elf symbol=LXC_ID
level=debug msg="Found variable with offset 128584" subsys=elf symbol=LXC_IPV4
level=debug msg="Found variable with offset 128568" subsys=elf symbol=LXC_IP_1
level=debug msg="Found variable with offset 128572" subsys=elf symbol=LXC_IP_2
level=debug msg="Found variable with offset 128576" subsys=elf symbol=LXC_IP_3
level=debug msg="Found variable with offset 128580" subsys=elf symbol=LXC_IP_4
level=debug msg="Found variable with offset 128592" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 128596" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 128608" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 128600" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 128604" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol with unknown section reference 19" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 162024" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 165908" subsys=elf symbol=cilium_calls_65535
level=debug msg="Found symbol with offset 162332" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 162310" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 162388" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 162086" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 162421" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 162119" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 161985" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 162161" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 162063" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 167800" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 162350" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 162436" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 162181" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 166331" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 165888" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 162146" subsys=elf symbol=cilium_signals
level=debug msg="Found variable with offset 128588" subsys=elf symbol=LXC_ID
level=debug msg="Found variable with offset 128584" subsys=elf symbol=LXC_IPV4
level=debug msg="Found variable with offset 128568" subsys=elf symbol=LXC_IP_1
level=debug msg="Found variable with offset 128572" subsys=elf symbol=LXC_IP_2
level=debug msg="Found variable with offset 128576" subsys=elf symbol=LXC_IP_3
level=debug msg="Found variable with offset 128580" subsys=elf symbol=LXC_IP_4
level=debug msg="Found variable with offset 128592" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 128596" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 128608" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 128600" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 128604" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol with unknown section reference 19" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 162024" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 165908" subsys=elf symbol=cilium_calls_65535
level=debug msg="Found symbol with offset 162332" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 162310" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 162388" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 162086" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 162421" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 162119" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 161985" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 162161" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 162063" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 167800" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 162350" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 162436" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 162181" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 166331" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 165888" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 162146" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 162286" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 162268" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping handle_policy" subsys=elf
level=debug msg="Skipping handle_to_container" subsys=elf
level=debug msg="Skipping handle_xgress" subsys=elf
level=debug msg="Skipping tail_handle_arp" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 162220" subsys=elf symbol=to-container
level=debug msg="Found section with offset 162237" subsys=elf symbol=from-container
level=debug msg="Found section with offset 162379" subsys=elf symbol=1/0xffff
level=debug msg="Found section with offset 164699" subsys=elf symbol=2/17
level=debug msg="Found section with offset 165346" subsys=elf symbol=2/6
level=debug msg="Found section with offset 166186" subsys=elf symbol=2/15
level=debug msg="Found section with offset 169343" subsys=elf symbol=2/1
level=debug msg="Found symbol with offset 162286" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 162268" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping handle_policy" subsys=elf
level=debug msg="Skipping handle_to_container" subsys=elf
level=debug msg="Skipping handle_xgress" subsys=elf
level=debug msg="Skipping tail_handle_arp" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 162220" subsys=elf symbol=to-container
level=debug msg="Found section with offset 162237" subsys=elf symbol=from-container
level=debug msg="Found section with offset 162379" subsys=elf symbol=1/0xffff
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=243_next/bpf_lxc.o subsys=elf template-path=/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de/bpf_lxc.o
level=debug msg="Found section with offset 164699" subsys=elf symbol=2/17
level=debug msg="Found section with offset 165346" subsys=elf symbol=2/6
level=debug msg="Found section with offset 166186" subsys=elf symbol=2/15
level=debug msg="Found section with offset 169343" subsys=elf symbol=2/1
level=debug msg="netlink: Replacing qdisc for lxc_health succeeded" subsys=datapath-loader
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=1909_next/bpf_lxc.o subsys=elf template-path=/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de/bpf_lxc.o
level=debug msg="netlink: Replacing qdisc for lxc76227b105fbe succeeded" subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=2.968387521s file-path=/var/run/cilium/state/templates/d570b2a3e940af53c376dab6c0be066b3c87c4e3/bpf_host.o subsys=datapath-loader
level=debug msg="Found variable with offset 160960" subsys=elf symbol=HOST_EP_ID
level=debug msg="Found variable with offset 160956" subsys=elf symbol=IPV4_NODEPORT
level=debug msg="Found variable with offset 160952" subsys=elf symbol=NATIVE_DEV_IFINDEX
level=debug msg="Found variable with offset 160964" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 160968" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 160980" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 160972" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 160976" subsys=elf symbol=SECLABEL_NB
level=debug msg="Watching template path" file-path=/var/run/cilium/state/templates/d570b2a3e940af53c376dab6c0be066b3c87c4e3/bpf_host.o subsys=datapath-loader
level=debug msg="Found symbol with unknown section reference 23" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 199744" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 203359" subsys=elf symbol=cilium_calls_hostns_65535
level=debug msg="Found symbol with offset 200082" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 200060" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 200125" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 199922" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 200158" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 199941" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 199705" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 199983" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 199899" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 205017" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 200100" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 200173" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 200003" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 203665" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 203339" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 199968" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 200036" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 200018" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping from_host" subsys=elf
level=debug msg="Skipping from_netdev" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_host" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_netdev" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Skipping to_host" subsys=elf
level=debug msg="Skipping to_netdev" subsys=elf
level=debug msg="Found section with offset 199806" subsys=elf symbol=to-netdev
level=debug msg="Found section with offset 199820" subsys=elf symbol=from-netdev
level=debug msg="Found section with offset 199877" subsys=elf symbol=to-host
level=debug msg="Found section with offset 199889" subsys=elf symbol=from-host
level=debug msg="Found section with offset 202133" subsys=elf symbol=2/17
level=debug msg="Found section with offset 202178" subsys=elf symbol=2/7
level=debug msg="Found section with offset 203556" subsys=elf symbol=2/15
level=debug msg="Found section with offset 205622" subsys=elf symbol=2/22
level=debug msg="Found section with offset 206429" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3398_next/bpf_host.o subsys=elf template-path=/var/run/cilium/state/templates/d570b2a3e940af53c376dab6c0be066b3c87c4e3/bpf_host.o
level=debug msg="Found variable with offset 160960" subsys=elf symbol=HOST_EP_ID
level=debug msg="Found variable with offset 160956" subsys=elf symbol=IPV4_NODEPORT
level=debug msg="Found variable with offset 160952" subsys=elf symbol=NATIVE_DEV_IFINDEX
level=debug msg="Found variable with offset 160964" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 160968" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 160980" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 160972" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 160976" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol with unknown section reference 23" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 199744" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 203359" subsys=elf symbol=cilium_calls_hostns_03398
level=debug msg="Found symbol with offset 200082" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 200060" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 200125" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 199922" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 200158" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 199941" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 199705" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 199983" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 199899" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 205017" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 200100" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 200173" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 200003" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 203665" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 203339" subsys=elf symbol=cilium_policy_03398
level=debug msg="Found symbol with offset 199968" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 200036" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 200018" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping from_host" subsys=elf
level=debug msg="Skipping from_netdev" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_host" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_netdev" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Skipping to_host" subsys=elf
level=debug msg="Skipping to_netdev" subsys=elf
level=debug msg="Found section with offset 199806" subsys=elf symbol=to-netdev
level=debug msg="Found section with offset 199820" subsys=elf symbol=from-netdev
level=debug msg="Found section with offset 199877" subsys=elf symbol=to-host
level=debug msg="Found section with offset 199889" subsys=elf symbol=from-host
level=debug msg="Found section with offset 202133" subsys=elf symbol=2/17
level=debug msg="Found section with offset 202178" subsys=elf symbol=2/7
level=debug msg="Found section with offset 203556" subsys=elf symbol=2/15
level=debug msg="Found section with offset 205622" subsys=elf symbol=2/22
level=debug msg="Found section with offset 206429" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3398_next/bpf_host_cilium_net.o subsys=elf template-path=3398_next/bpf_host.o
level=debug msg="Found variable with offset 160960" subsys=elf symbol=HOST_EP_ID
level=debug msg="Found variable with offset 160956" subsys=elf symbol=IPV4_NODEPORT
level=debug msg="Found variable with offset 160952" subsys=elf symbol=NATIVE_DEV_IFINDEX
level=debug msg="Found variable with offset 160964" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 160968" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 160980" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 160972" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 160976" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol with unknown section reference 23" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 199744" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 203359" subsys=elf symbol=cilium_calls_hostns_03398
level=debug msg="Found symbol with offset 200082" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 200060" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 200125" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 199922" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 200158" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 199941" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 199705" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 199983" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 199899" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 205017" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 200100" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 200173" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 200003" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 203665" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 203339" subsys=elf symbol=cilium_policy_03398
level=debug msg="Found symbol with offset 199968" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 200036" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 200018" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping from_host" subsys=elf
level=debug msg="Skipping from_netdev" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_host" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_netdev" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Skipping to_host" subsys=elf
level=debug msg="Skipping to_netdev" subsys=elf
level=debug msg="Found section with offset 199806" subsys=elf symbol=to-netdev
level=debug msg="Found section with offset 199820" subsys=elf symbol=from-netdev
level=debug msg="Found section with offset 199877" subsys=elf symbol=to-host
level=debug msg="Found section with offset 199889" subsys=elf symbol=from-host
level=debug msg="Found section with offset 202133" subsys=elf symbol=2/17
level=debug msg="Found section with offset 202178" subsys=elf symbol=2/7
level=debug msg="Found section with offset 203556" subsys=elf symbol=2/15
level=debug msg="Found section with offset 205622" subsys=elf symbol=2/22
level=debug msg="Found section with offset 206429" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3398_next/bpf_netdev_eth0.o subsys=elf template-path=3398_next/bpf_host.o
level=debug msg="netlink: Replacing qdisc for cilium_host succeeded" subsys=datapath-loader
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="netlink: Replacing qdisc for cilium_host succeeded" subsys=datapath-loader
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 31.321µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/243 subsys=endpoint temporaryDirectory=243_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 directory=243_next_fail endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=653.370712ms bpfWaitForELF=1.779702602s bpfWriteELF="455.446µs" buildDuration=2.439412459s containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ mapSync="95.719µs" policyCalculation="607.65µs" prepareBuild="484.771µs" proxyConfiguration="11.118µs" proxyPolicyCalculation="296.436µs" proxyWaitForAck="72.719µs" reason="updated security labels" subsys=endpoint waitingForCTClean=1.485012ms waitingForLock="2.699µs"
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=waiting-to-regenerate identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="Controller func execution time: 71.321µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="248.046µs" eventEnqueueWaitTime="17.547µs" eventHandlingDuration=2.440175068s eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-243 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2020-07-18 11:28:34.8590479 +0000 UTC m=+15.650445242" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=regenerating identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=4 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=4 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"2dce9b4f5db77738efd141422d7ef23a011c3b0c\")" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 27.88µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_stale endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=243_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/243 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next_fail endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_stale endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 44.484µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=ready identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=1.783129ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ mapSync="27.605µs" policyCalculation="76.923µs" prepareBuild="257.035µs" proxyConfiguration="15.711µs" proxyPolicyCalculation="264.808µs" proxyWaitForAck="52.277µs" reason="one or more identities created or deleted" subsys=endpoint waitingForCTClean=766ns waitingForLock="2.574µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=ready identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=2.210960717s eventEnqueueWaitTime=621ns eventHandlingDuration=2.208637ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-243 subsys=eventqueue
level=debug msg="Controller func execution time: 103.581µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="netlink: Replacing qdisc for cilium_net succeeded" subsys=datapath-loader
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 54.67µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/1909 subsys=endpoint temporaryDirectory=1909_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 directory=1909_next_fail endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration" bpfCompilation=2.547552033s bpfLoadProg=673.483998ms bpfWaitForELF=2.547861685s bpfWriteELF="437.909µs" buildDuration=3.231719927s containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ mapSync="124.264µs" policyCalculation="109.238µs" prepareBuild="905.992µs" proxyConfiguration="10.486µs" proxyPolicyCalculation="52.475µs" proxyWaitForAck="554.47µs" reason="updated security labels" subsys=endpoint waitingForCTClean="679.176µs" waitingForLock="2.78µs"
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=waiting-to-regenerate identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="170.634µs" eventEnqueueWaitTime="1.58µs" eventHandlingDuration=3.232064903s eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-1909 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2020-07-18 11:28:34.880669784 +0000 UTC m=+15.672067130" subsys=endpoint
level=debug msg="End of create request" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=info msg="Successful endpoint creation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=debug msg="Controller func execution time: 200.434µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=regenerating identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=4 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=4 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"3e1471c95af8d4fc835e5059d5736bb8f7d4f060\")" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 19.104µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_stale endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=1909_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/1909 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next_fail endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_stale endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 97.953µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=ready identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=1.886677ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ mapSync="21.302µs" policyCalculation="87.278µs" prepareBuild="294.711µs" proxyConfiguration="18.157µs" proxyPolicyCalculation="67.254µs" proxyWaitForAck="47.52µs" reason="one or more identities created or deleted" subsys=endpoint waitingForCTClean=817ns waitingForLock="2.938µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=ready identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=2.232657182s eventEnqueueWaitTime="1.322µs" eventHandlingDuration=2.134352ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-1909 subsys=eventqueue
level=debug msg="Controller func execution time: 99.91µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="netlink: Replacing qdisc for eth0 succeeded" subsys=datapath-loader
level=debug msg="netlink: Replacing qdisc for eth0 succeeded" subsys=datapath-loader
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 62.556µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3398 subsys=endpoint temporaryDirectory=3398_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 directory=3398_next_fail endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration" bpfCompilation=2.968387521s bpfLoadProg=1.092518762s bpfWaitForELF=2.968818829s bpfWriteELF="434.869µs" buildDuration=4.070612406s containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ mapSync="39.08µs" policyCalculation="124.565µs" prepareBuild="829.062µs" proxyConfiguration="9.756µs" proxyPolicyCalculation="2.977µs" proxyWaitForAck="87.264µs" reason="updated security labels" subsys=endpoint waitingForCTClean="655.783µs" waitingForLock="2.009µs"
level=debug msg="Controller func execution time: 171.964µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="692.358µs" eventEnqueueWaitTime="1.351µs" eventHandlingDuration=4.071014289s eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3398 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2020-07-18 11:28:35.387840384 +0000 UTC m=+16.179237700" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=regenerating identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="BPF header file hashed (was: \"47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3\")" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 14.11µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_stale endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=3398_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3398 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next_fail endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_stale endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 15.109µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration="846.736µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ mapSync="1.789µs" policyCalculation="41.701µs" prepareBuild="178.581µs" proxyConfiguration="10.655µs" proxyPolicyCalculation="3.775µs" proxyWaitForAck="28.813µs" reason="one or more identities created or deleted" subsys=endpoint waitingForCTClean=363ns waitingForLock="1.705µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=3.741155371s eventEnqueueWaitTime=633ns eventHandlingDuration=1.025402ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3398 subsys=eventqueue
level=debug msg="Controller func execution time: 55.621µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipped ipcache map update on pod add" error="empty PodIPs" hostIP= k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp podIP= podIPs="[]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="empty PodIPs" hostIP= k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s podIP= podIPs="[]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="empty PodIPs" hostIP= k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp podIP= podIPs="[]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="empty PodIPs" hostIP= k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s podIP= podIPs="[]" subsys=k8s-watcher
level=debug msg="Controller func execution time: 17.799µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="GET /config request" params="{HTTPRequest:0xc000a8eb00}" subsys=daemon
level=debug msg="GET /config request" params="{HTTPRequest:0xc0011f8600}" subsys=daemon
level=debug msg="GET /config request" params="{HTTPRequest:0xc000441300}" subsys=daemon
level=debug msg="Allocated random IP" ip=10.10.1.141 owner=kube-system/coredns-66bff467f8-f54bp subsys=ipam
level=debug msg="GET /config request" params="{HTTPRequest:0xc000a8ef00}" subsys=daemon
level=debug msg="PUT /endpoint/{id} request" endpoint="{Addressing:0xc00064ac80 ContainerID:94c531c242ae04b606dea9d7cf01086136d2c70b4717953a802a472985638ab8 ContainerName: DatapathConfiguration:<nil> DatapathMapID:0 DockerEndpointID: DockerNetworkID: HostMac:4a:bf:b9:62:18:37 ID:0 InterfaceIndex:11 InterfaceName:lxcaa973561e178 K8sNamespace:kube-system K8sPodName:coredns-66bff467f8-f54bp Labels:[] Mac:96:1d:ae:02:c7:97 Pid:0 PolicyEnabled:false State:waiting-for-identity SyncBuildEndpoint:true}" subsys=daemon
level=info msg="Create endpoint request" addressing="&{10.10.1.141 d1c24518-c8e9-11ea-9724-8aaf378a454f  }" containerID=94c531c242ae04b606dea9d7cf01086136d2c70b4717953a802a472985638ab8 datapathConfiguration="<nil>" interface=lxcaa973561e178 k8sPodName=kube-system/coredns-66bff467f8-f54bp labels="[]" subsys=daemon sync-build=true
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="New create request" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Namespace" subsys=k8s-watcher
level=debug msg="Connecting to k8s local stores to retrieve labels for pod" k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp subsys=k8s
level=debug msg="No sidecar.istio.io/status annotation" k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp subsys=k8s
level=debug msg="Starting new controller" name=endpoint-3097-regeneration-recovery subsys=controller uuid=d1c31f34-c8e9-11ea-9724-8aaf378a454f
level=debug msg="creating new EventQueue" name=endpoint-3097 numBufferedEvents=25 subsys=eventqueue
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="37.187µs" eventEnqueueWaitTime=725ns eventHandlingDuration="40.806µs" eventType="*endpoint.EndpointPolicyVisibilityEvent" name=endpoint-3097 subsys=eventqueue
level=debug msg="Refreshing labels of endpoint" containerID=94c531c242 endpointID=3097 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" infoLabels="k8s:pod-template-hash=66bff467f8" subsys=endpoint
level=debug msg="Assigning information label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ obj="{Key:pod-template-hash Value:66bff467f8 Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ obj="{Key:k8s-app Value:kube-dns Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ obj="{Key:io.kubernetes.pod.namespace Value:kube-system Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ obj="{Key:io.cilium.k8s.policy.serviceaccount Value:coredns Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ obj="{Key:io.cilium.k8s.policy.cluster Value:default Source:k8s}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" subsys=identity-cache
level=debug msg="Allocating key" key="[k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns]" subsys=allocator
level=debug msg="Allocating new master ID" key="k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=coredns;k8s:io.kubernetes.pod.namespace=kube-system;k8s:k8s-app=kube-dns;" subsys=allocator
level=info msg="Skipped non-kubernetes labels when labelling ciliumidentity. All labels will still be used in identity determination" labels="map[]" subsys=crd-allocator
level=debug msg="Getting CEP during an initialization" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=info msg="Allocated new global key" key="k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=coredns;k8s:io.kubernetes.pod.namespace=kube-system;k8s:k8s-app=kube-dns;" subsys=allocator
level=debug msg="Allocated key" id=39246 key="[k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns]" subsys=allocator
level=debug msg="Resolved identity" identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" isNew=true isNewLocally=true subsys=identity-cache
level=debug msg="UpdateIdentities: Adding a new identity" identity=39246 labels="[k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns]" subsys=policy
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=39246 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 endpointState=ready ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 endpointState=waiting-to-regenerate identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-3097 subsys=controller uuid=d1c4b059-c8e9-11ea-9724-8aaf378a454f
level=info msg="Waiting for endpoint to be generated" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 67.035µs" name=resolve-identity-3097 subsys=controller uuid=d1c4b059-c8e9-11ea-9724-8aaf378a454f
level=debug msg="UpdateIdentities: Skipping add of an existing identical identity" identity=39246 subsys=policy
level=debug msg="Regenerating all endpoints" subsys=daemon
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=debug msg="Skipped duplicate endpoint regeneration level no-rebuild trigger due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 endpointState=waiting-to-regenerate identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ reason="updated security labels" startTime="2020-07-18 11:28:36.049189979 +0000 UTC m=+16.840587278" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 endpointState=regenerating identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=3097_next endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_03097 subsys=bpf
level=debug msg="flushing old PolicyMap" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="deleting all entries in map" file-path=/sys/fs/bpf/tc/globals/cilium_policy_03097 name=cilium_policy_03097 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 forcedRegeneration=true identity=39246 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 13288 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 594 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=4 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="inserting resource into cache" subsys=xds xdsCachedVersion=4 xdsResourceName=10.10.1.141 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="committing cache transaction and notifying of new version" subsys=xds xdsCachedVersion=4 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=waiting-to-regenerate identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=f5ee91760d77b8fb41ebbd8e7b1550ade5357c70 containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 file-path=3097_next/ep_config.h identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=waiting-to-regenerate identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2020-07-18 11:28:36.049981611 +0000 UTC m=+16.841378923" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=regenerating identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=5 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2020-07-18 11:28:36.050242969 +0000 UTC m=+16.841640285" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=regenerating identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2020-07-18 11:28:36.050548596 +0000 UTC m=+16.841945922" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=regenerating identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=5 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"3e1471c95af8d4fc835e5059d5736bb8f7d4f060\")" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 16.91µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_stale endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=1909_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/1909 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next_fail endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_stale endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 19.391µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=ready identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=1.558281ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ mapSync="22.044µs" policyCalculation="52.285µs" prepareBuild="165.047µs" proxyConfiguration="13.932µs" proxyPolicyCalculation="644.667µs" proxyWaitForAck="42.492µs" reason="one or more identities created or deleted" subsys=endpoint waitingForCTClean=492ns waitingForLock="2.506µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=ready identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="200.166µs" eventEnqueueWaitTime="1.677µs" eventHandlingDuration=1.842865ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-1909 subsys=eventqueue
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Controller func execution time: 48.653µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=5 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=5 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"2dce9b4f5db77738efd141422d7ef23a011c3b0c\")" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 17.126µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_stale endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=243_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/243 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next_fail endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_stale endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 11.937µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=ready identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=2.924756ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ mapSync="19.098µs" policyCalculation="44.966µs" prepareBuild="165.728µs" proxyConfiguration="11.055µs" proxyPolicyCalculation=2.029957ms proxyWaitForAck="35.404µs" reason="one or more identities created or deleted" subsys=endpoint waitingForCTClean=878ns waitingForLock="2.062µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=ready identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="282.052µs" eventEnqueueWaitTime="1.472µs" eventHandlingDuration=3.079706ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-243 subsys=eventqueue
level=debug msg="BPF header file hashed (was: \"47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3\")" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 16.077µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_stale endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=3398_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3398 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next_fail endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_stale endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 18.871µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=3.541729ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ mapSync="1.431µs" policyCalculation="75.241µs" prepareBuild="194.911µs" proxyConfiguration="11.289µs" proxyPolicyCalculation=2.552375ms proxyWaitForAck="34.087µs" reason="one or more identities created or deleted" subsys=endpoint waitingForCTClean=483ns waitingForLock="1.447µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="19.391µs" eventEnqueueWaitTime="1.557µs" eventHandlingDuration=3.73041ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3398 subsys=eventqueue
level=debug msg="Controller func execution time: 81.58µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 50.245µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Found variable with offset 128588" subsys=elf symbol=LXC_ID
level=debug msg="Found variable with offset 128584" subsys=elf symbol=LXC_IPV4
level=debug msg="Found variable with offset 128568" subsys=elf symbol=LXC_IP_1
level=debug msg="Found variable with offset 128572" subsys=elf symbol=LXC_IP_2
level=debug msg="Found variable with offset 128576" subsys=elf symbol=LXC_IP_3
level=debug msg="Found variable with offset 128580" subsys=elf symbol=LXC_IP_4
level=debug msg="Found variable with offset 128592" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 128596" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 128608" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 128600" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 128604" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol with unknown section reference 19" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 162024" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 165908" subsys=elf symbol=cilium_calls_65535
level=debug msg="Found symbol with offset 162332" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 162310" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 162388" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 162086" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 162421" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 162119" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 161985" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 162161" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 162063" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 167800" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 162350" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 162436" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 162181" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 166331" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 165888" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 162146" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 162286" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 162268" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping handle_policy" subsys=elf
level=debug msg="Skipping handle_to_container" subsys=elf
level=debug msg="Skipping handle_xgress" subsys=elf
level=debug msg="Skipping tail_handle_arp" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 162220" subsys=elf symbol=to-container
level=debug msg="Found section with offset 162237" subsys=elf symbol=from-container
level=debug msg="Found section with offset 162379" subsys=elf symbol=1/0xffff
level=debug msg="Found section with offset 164699" subsys=elf symbol=2/17
level=debug msg="Found section with offset 165346" subsys=elf symbol=2/6
level=debug msg="Found section with offset 166186" subsys=elf symbol=2/15
level=debug msg="Found section with offset 169343" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3097_next/bpf_lxc.o subsys=elf template-path=/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de/bpf_lxc.o
level=debug msg="Upserting IP into ipcache layer" identity="{unmanaged custom-resource false}" ipAddr=10.10.1.141 k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp key=0 namedPorts="map[dns:{53 17} dns-tcp:{53 6} metrics:{9153 6}]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=unmanaged ipAddr="{10.10.1.141 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Artificially increasing policy revision to enforce policy recalculation" subsys=daemon
level=debug msg="Controller func execution time: 37.95475ms" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="netlink: Replacing qdisc for lxcaa973561e178 succeeded" subsys=datapath-loader
level=debug msg="Allocated random IP" ip=10.10.1.115 owner=kube-system/coredns-66bff467f8-8mx7s subsys=ipam
level=debug msg="PUT /endpoint/{id} request" endpoint="{Addressing:0xc0008fa740 ContainerID:35161547eb248f7de4a4cd8ca9ddf7198c4b770cb513d31ed647257c5575a631 ContainerName: DatapathConfiguration:<nil> DatapathMapID:0 DockerEndpointID: DockerNetworkID: HostMac:16:1d:1a:87:60:27 ID:0 InterfaceIndex:13 InterfaceName:lxcc41506f1c156 K8sNamespace:kube-system K8sPodName:coredns-66bff467f8-8mx7s Labels:[] Mac:ae:bb:b7:34:35:3b Pid:0 PolicyEnabled:false State:waiting-for-identity SyncBuildEndpoint:true}" subsys=daemon
level=info msg="Create endpoint request" addressing="&{10.10.1.115 d1cca062-c8e9-11ea-9724-8aaf378a454f  }" containerID=35161547eb248f7de4a4cd8ca9ddf7198c4b770cb513d31ed647257c5575a631 datapathConfiguration="<nil>" interface=lxcc41506f1c156 k8sPodName=kube-system/coredns-66bff467f8-8mx7s labels="[]" subsys=daemon sync-build=true
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="New create request" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Namespace" subsys=k8s-watcher
level=debug msg="Connecting to k8s local stores to retrieve labels for pod" k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s subsys=k8s
level=debug msg="No sidecar.istio.io/status annotation" k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s subsys=k8s
level=debug msg="Starting new controller" name=endpoint-945-regeneration-recovery subsys=controller uuid=d1cd746f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="creating new EventQueue" name=endpoint-945 numBufferedEvents=25 subsys=eventqueue
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s-watcher
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="38.286µs" eventEnqueueWaitTime="1.123µs" eventHandlingDuration="46.054µs" eventType="*endpoint.EndpointPolicyVisibilityEvent" name=endpoint-945 subsys=eventqueue
level=debug msg="Refreshing labels of endpoint" containerID=35161547eb endpointID=945 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" infoLabels="k8s:pod-template-hash=66bff467f8" subsys=endpoint
level=debug msg="Assigning information label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ obj="{Key:pod-template-hash Value:66bff467f8 Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ obj="{Key:io.kubernetes.pod.namespace Value:kube-system Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ obj="{Key:io.cilium.k8s.policy.serviceaccount Value:coredns Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ obj="{Key:io.cilium.k8s.policy.cluster Value:default Source:k8s}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ obj="{Key:k8s-app Value:kube-dns Source:k8s}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" subsys=identity-cache
level=debug msg="Allocating key" key="[k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns]" subsys=allocator
level=debug msg="Resolved identity" identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" isNew=false isNewLocally=false subsys=identity-cache
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=39246 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 endpointState=ready ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 endpointState=waiting-to-regenerate identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-945 subsys=controller uuid=d1ce1940-c8e9-11ea-9724-8aaf378a454f
level=info msg="Waiting for endpoint to be generated" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 identityLabels="k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=coredns,k8s:io.kubernetes.pod.namespace=kube-system,k8s:k8s-app=kube-dns" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 60.846µs" name=resolve-identity-945 subsys=controller uuid=d1ce1940-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Getting CEP during an initialization" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ reason="updated security labels" startTime="2020-07-18 11:28:36.110621716 +0000 UTC m=+16.902019033" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 endpointState=regenerating identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=945_next endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_00945 subsys=bpf
level=debug msg="flushing old PolicyMap" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="deleting all entries in map" file-path=/sys/fs/bpf/tc/globals/cilium_policy_00945 name=cilium_policy_00945 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 forcedRegeneration=true identity=39246 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 16546 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 953 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=5 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="inserting resource into cache" subsys=xds xdsCachedVersion=5 xdsResourceName=10.10.1.115 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="committing cache transaction and notifying of new version" subsys=xds xdsCachedVersion=5 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=c38772710424c5948f84db461c74e567e8452ff7 containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 file-path=945_next/ep_config.h identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Found variable with offset 128588" subsys=elf symbol=LXC_ID
level=debug msg="Found variable with offset 128584" subsys=elf symbol=LXC_IPV4
level=debug msg="Found variable with offset 128568" subsys=elf symbol=LXC_IP_1
level=debug msg="Found variable with offset 128572" subsys=elf symbol=LXC_IP_2
level=debug msg="Found variable with offset 128576" subsys=elf symbol=LXC_IP_3
level=debug msg="Found variable with offset 128580" subsys=elf symbol=LXC_IP_4
level=debug msg="Found variable with offset 128592" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 128596" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 128608" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 128600" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 128604" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol with unknown section reference 19" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Found symbol with offset 162024" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 165908" subsys=elf symbol=cilium_calls_65535
level=debug msg="Found symbol with offset 162332" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 162310" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 162388" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 162086" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 162421" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 162119" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 161985" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 162161" subsys=elf symbol=cilium_lb4_backends
level=debug msg="Found symbol with offset 162063" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 167800" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 162350" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 162436" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 162181" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 166331" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 165888" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 162146" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 162286" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 162268" subsys=elf symbol=cilium_tunnel_map
level=debug msg="Skipping handle_policy" subsys=elf
level=debug msg="Skipping handle_to_container" subsys=elf
level=debug msg="Skipping handle_xgress" subsys=elf
level=debug msg="Skipping tail_handle_arp" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 162220" subsys=elf symbol=to-container
level=debug msg="Found section with offset 162237" subsys=elf symbol=from-container
level=debug msg="Found section with offset 162379" subsys=elf symbol=1/0xffff
level=debug msg="Found section with offset 164699" subsys=elf symbol=2/17
level=debug msg="Found section with offset 165346" subsys=elf symbol=2/6
level=debug msg="Found section with offset 166186" subsys=elf symbol=2/15
level=debug msg="Found section with offset 169343" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=945_next/bpf_lxc.o subsys=elf template-path=/var/run/cilium/state/templates/83585a6f3708305546ef9679a0979f5de669a8de/bpf_lxc.o
level=debug msg="netlink: Replacing qdisc for lxcc41506f1c156 succeeded" subsys=datapath-loader
level=debug msg="Controller func execution time: 43.674092ms" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Upserting IP into ipcache layer" identity="{unmanaged custom-resource false}" ipAddr=10.10.1.115 k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s key=0 namedPorts="map[dns:{53 17} dns-tcp:{53 6} metrics:{9153 6}]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=unmanaged ipAddr="{10.10.1.115 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Greeting successful" host="http://10.10.1.241:4240" ipAddr=10.10.1.241 nodeName=kind-worker path="Via L3" rtt=3.037876735s subsys=health-server
level=debug msg="Run complete" subsys=health-server
level=debug msg="Sending request for /cluster/nodes ..." subsys=health-server
level=debug msg="Got cilium /cluster/nodes" subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=info msg="Serving cilium health at unix:///var/run/cilium/health.sock" subsys=health-server
level=debug msg="Probe successful" ipAddr=172.18.0.2 nodeName=kind-worker rtt="324.181µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=10.10.1.241 nodeName=kind-worker rtt="404.448µs" subsys=health-server
level=debug msg="Skipped ipcache map update on pod add" error="ipcache entry for podIP 10.10.1.185 owned by kvstore or agent" hostIP=10.10.1.185 k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-67795f75bd-lptkp podIP=10.10.1.185 podIPs="[{10.10.1.185}]" subsys=k8s-watcher
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 46.071µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3097 subsys=endpoint temporaryDirectory=3097_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 directory=3097_next_fail endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-3097 subsys=controller uuid=d22d5161-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3097_next endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=676.353956ms bpfWaitForELF="7.163µs" bpfWriteELF="442.275µs" buildDuration=685.144861ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ mapSync="54.955µs" policyCalculation="81.324µs" prepareBuild="483.756µs" proxyConfiguration="7.977µs" proxyPolicyCalculation="43.933µs" proxyWaitForAck="70.712µs" reason="updated security labels" subsys=endpoint waitingForCTClean=4.57512ms waitingForLock="1.909µs"
level=debug msg="Controller func execution time: 162.553µs" name=sync-policymap-3097 subsys=controller uuid=d22d5161-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="687.272µs" eventEnqueueWaitTime="1.42µs" eventHandlingDuration=685.62289ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3097 subsys=eventqueue
level=debug msg="End of create request" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=info msg="Successful endpoint creation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 14.867µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/945 subsys=endpoint temporaryDirectory=945_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=2 directory=945_next_fail endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-945 subsys=controller uuid=d23a276f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=945_next endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=697.55829ms bpfWaitForELF="6.035µs" bpfWriteELF="653.677µs" buildDuration=707.801264ms containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ mapSync="70.546µs" policyCalculation="114.183µs" prepareBuild=1.233938ms proxyConfiguration="11.568µs" proxyPolicyCalculation="54.975µs" proxyWaitForAck="28.664µs" reason="updated security labels" subsys=endpoint waitingForCTClean="886.976µs" waitingForLock="2.028µs"
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=200
level=debug msg="End of create request" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="443.326µs" eventEnqueueWaitTime="1.843µs" eventHandlingDuration=707.99406ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-945 subsys=eventqueue
level=info msg="Successful endpoint creation" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=daemon
level=debug msg="Controller func execution time: 60.413µs" name=sync-policymap-945 subsys=controller uuid=d23a276f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Regenerating all endpoints" subsys=daemon
level=info msg="regenerating all endpoints" reason="Named ports added or updated" subsys=endpoint-manager
level=debug msg="Triggering endpoint regeneration due to Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=waiting-to-regenerate identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Triggering endpoint regeneration due to Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=waiting-to-regenerate identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Triggering endpoint regeneration due to Named ports added or updated" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 endpointState=waiting-to-regenerate identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ reason="Named ports added or updated" startTime="2020-07-18 11:28:37.049566412 +0000 UTC m=+17.840963731" subsys=endpoint
level=debug msg="Regenerating endpoint: Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 endpointState=regenerating identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 forcedRegeneration=false identity=26160 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 15870 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 498 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"3e1471c95af8d4fc835e5059d5736bb8f7d4f060\")" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=3e1471c95af8d4fc835e5059d5736bb8f7d4f060 containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 14.964µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=1909_stale endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=1909_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/1909 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=1909_next_fail endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=1909_stale endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 18.076µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=1909_next endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 endpointState=ready identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration="985.909µs" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ mapSync="22.725µs" policyCalculation="103.257µs" prepareBuild="186.04µs" proxyConfiguration="9.516µs" proxyPolicyCalculation="69.302µs" proxyWaitForAck="37.854µs" reason="Named ports added or updated" subsys=endpoint waitingForCTClean=481ns waitingForLock="1.862µs"
level=debug msg="Successfully regenerated endpoint program (Reason: Named ports added or updated)" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 endpointState=ready identity=26160 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="58.227µs" eventEnqueueWaitTime="1.345µs" eventHandlingDuration=1.165789ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-1909 subsys=eventqueue
level=debug msg="Triggering endpoint regeneration due to Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 endpointState=waiting-to-regenerate identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Controller func execution time: 56.461µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Triggering endpoint regeneration due to Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ reason="Named ports added or updated" startTime="2020-07-18 11:28:37.050972478 +0000 UTC m=+17.842369798" subsys=endpoint
level=debug msg="Regenerating endpoint: Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 endpointState=regenerating identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3097_next endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 forcedRegeneration=false identity=39246 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 6500 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 392 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"f5ee91760d77b8fb41ebbd8e7b1550ade5357c70\")" bpfHeaderfileHash=f5ee91760d77b8fb41ebbd8e7b1550ade5357c70 containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=f5ee91760d77b8fb41ebbd8e7b1550ade5357c70 containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 14.83µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=3097_stale endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=3097_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3097 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=3097_next_fail endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=3097_stale endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-3097 subsys=controller uuid=d22d5161-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 17.403µs" name=sync-policymap-3097 subsys=controller uuid=d22d5161-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=3097_next endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration="912.624µs" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ mapSync="18.781µs" policyCalculation="85.411µs" prepareBuild="142.057µs" proxyConfiguration="9.919µs" proxyPolicyCalculation="53.857µs" proxyWaitForAck="30.753µs" reason="Named ports added or updated" subsys=endpoint waitingForCTClean=440ns waitingForLock="1.439µs"
level=debug msg="Successfully regenerated endpoint program (Reason: Named ports added or updated)" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="139.315µs" eventEnqueueWaitTime="1.065µs" eventHandlingDuration=1.086798ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3097 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ reason="Named ports added or updated" startTime="2020-07-18 11:28:37.052087161 +0000 UTC m=+17.843484486" subsys=endpoint
level=debug msg="Regenerating endpoint: Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 endpointState=regenerating identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 forcedRegeneration=false identity=4 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 13316 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 397 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"2dce9b4f5db77738efd141422d7ef23a011c3b0c\")" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=2dce9b4f5db77738efd141422d7ef23a011c3b0c containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 14.317µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=243_stale endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=243_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/243 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=243_next_fail endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=243_stale endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 16.687µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=243_next endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 endpointState=ready identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration="861.575µs" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ mapSync="19.244µs" policyCalculation="78.455µs" prepareBuild="158.436µs" proxyConfiguration="9.41µs" proxyPolicyCalculation="53.48µs" proxyWaitForAck="30.995µs" reason="Named ports added or updated" subsys=endpoint waitingForCTClean=499ns waitingForLock="1.484µs"
level=debug msg="Successfully regenerated endpoint program (Reason: Named ports added or updated)" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 endpointState=ready identity=4 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="7.201µs" eventEnqueueWaitTime="1.486µs" eventHandlingDuration=3.64629ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-243 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ reason="Named ports added or updated" startTime="2020-07-18 11:28:37.053146938 +0000 UTC m=+17.844544257" subsys=endpoint
level=debug msg="Regenerating endpoint: Named ports added or updated" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 endpointState=regenerating identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 forcedRegeneration=false identity=1 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 12977 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 0 0}}} {0 0 <nil>} 395 0}"
level=debug msg="BPF header file hashed (was: \"47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3\")" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=47528707ad7bc1e9c459d189f5ba2d1e11c5a8c3 containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 13.515µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=3398_stale endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=3398_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3398 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=3398_next_fail endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=2 directory=3398_stale endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 14.72µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=3398_next endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration="759.569µs" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ mapSync="3.478µs" policyCalculation="77.792µs" prepareBuild="140.409µs" proxyConfiguration="9.481µs" proxyPolicyCalculation="2.44µs" proxyWaitForAck="27.736µs" reason="Named ports added or updated" subsys=endpoint waitingForCTClean=392ns waitingForLock="1.431µs"
level=debug msg="Successfully regenerated endpoint program (Reason: Named ports added or updated)" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=2.180256ms eventEnqueueWaitTime="1.015µs" eventHandlingDuration="919.301µs" eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3398 subsys=eventqueue
level=debug msg="Controller func execution time: 58.243µs" name=sync-policymap-3097 subsys=controller uuid=d22d5161-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ reason="Named ports added or updated" startTime="2020-07-18 11:28:37.054174814 +0000 UTC m=+17.845572130" subsys=endpoint
level=debug msg="Regenerating endpoint: Named ports added or updated" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 endpointState=regenerating identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=945_next endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=2 policyRevision.repo=2 subsys=endpoint
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=6 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"c38772710424c5948f84db461c74e567e8452ff7\")" bpfHeaderfileHash=c38772710424c5948f84db461c74e567e8452ff7 containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=c38772710424c5948f84db461c74e567e8452ff7 containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 14.577µs" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=945_stale endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=945_stale containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/945 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=945_next_fail endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=945_stale endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-945 subsys=controller uuid=d23a276f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller update time: 14.467µs" name=sync-policymap-945 subsys=controller uuid=d23a276f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="removing directory" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 directory=945_next endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration="760.621µs" containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ mapSync="14.524µs" policyCalculation="35.354µs" prepareBuild="134.421µs" proxyConfiguration="8.72µs" proxyPolicyCalculation="53.816µs" proxyWaitForAck="29.1µs" reason="Named ports added or updated" subsys=endpoint waitingForCTClean=368ns waitingForLock="1.38µs"
level=debug msg="Successfully regenerated endpoint program (Reason: Named ports added or updated)" code=OK containerID= datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 endpointState=ready identity=39246 ipv4= ipv6= k8sPodName=/ policyRevision=2 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="68.5µs" eventEnqueueWaitTime="1.18µs" eventHandlingDuration=5.472569ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-945 subsys=eventqueue
level=debug msg="Controller func execution time: 53.979µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 44.222µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 53.963µs" name=sync-policymap-945 subsys=controller uuid=d23a276f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipped ipcache map update on pod add" error="ipcache entry for podIP 10.10.1.115 owned by kvstore or agent" hostIP=10.10.1.115 k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s podIP=10.10.1.115 podIPs="[{10.10.1.115}]" subsys=k8s-watcher
level=debug msg="Skipped ipcache map update on pod add" error="ipcache entry for podIP 10.10.1.141 owned by kvstore or agent" hostIP=10.10.1.141 k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp podIP=10.10.1.141 podIPs="[{10.10.1.141}]" subsys=k8s-watcher
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints="10.10.1.141:53/TCP,10.10.1.141:53/UDP,10.10.1.141:9153/TCP" k8sNamespace=kube-system k8sSvcName=kube-dns old-service=nil service="frontend:10.11.0.10/ports=[dns dns-tcp metrics]/selector=map[k8s-app:kube-dns]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[{0 kind-worker {10.10.1.141 {UDP 53} 0}}]" serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[{0 kind-worker {10.10.1.141 {UDP 53} 0}}]" serviceID=2 serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=2 subsys=service
level=debug msg="Adding new backend" backendID=2 backends="[{0 kind-worker {10.10.1.141 {UDP 53} 0}}]" l3n4Addr="{10.10.1.141 {UDP 53} 0}" serviceID=2 serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserting service entry" slaveSlot=1 subsys=map-lb svcKey="10.11.0.10:53" svcVal="2 (2) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.10:53" svcVal="0 (2) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserting service" backends="[{0 kind-worker {10.10.1.141 {TCP 9153} 0}}]" serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[{0 kind-worker {10.10.1.141 {TCP 9153} 0}}]" serviceID=1 serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=1 subsys=service
level=debug msg="Adding new backend" backendID=3 backends="[{0 kind-worker {10.10.1.141 {TCP 9153} 0}}]" l3n4Addr="{10.10.1.141 {TCP 9153} 0}" serviceID=1 serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserting service entry" slaveSlot=1 subsys=map-lb svcKey="10.11.0.10:9153" svcVal="3 (1) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.10:9153" svcVal="0 (1) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=1 subsys=service
level=debug msg="Controller func execution time: 134.318µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Received node update event from custom-resource: types.Node{Name:\"kind-control-plane\", Cluster:\"default\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x12, 0x0, 0x3}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xa, 0x0, 0xb3}}}, IPv4AllocCIDR:(*cidr.CIDR)(0xc00000e828), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xa, 0x0, 0x54}, IPv6HealthIP:net.IP(nil), ClusterID:0, Source:\"custom-resource\", EncryptionKey:0x0, Labels:map[string]string(nil)}" subsys=nodemanager
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource false}" ipAddr=172.18.0.3 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=remote-node ipAddr="{172.18.0.3 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource false}" ipAddr=10.10.0.179 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=remote-node ipAddr="{10.10.0.179 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{health custom-resource false}" ipAddr=10.10.0.84 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=health ipAddr="{10.10.0.84 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg=insertNeighbor HardwareAddr="02:42:ac:12:00:03" Interface=eth0 LinkIndex=60 ipAddr=172.18.0.3 reason="insertNeighbor NeighSet" subsys=linux-datapath
level=debug msg="Updating tunnel map entry" allocCIDR=10.10.0.0/24 ipAddr=172.18.0.3 subsys=linux-datapath
level=debug msg="Updating tunnel map entry" endpoint=172.18.0.3 key=0 prefix=10.10.0.0 subsys=map-tunnel
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 111.419µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 7.405464ms" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Upserting IP into ipcache layer" identity="{26160 custom-resource false}" ipAddr=10.10.1.185 k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-67795f75bd-lptkp key=0 namedPorts="map[]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=26160 ipAddr="{10.10.1.185 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 85.784µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints="10.10.1.115:53/TCP,10.10.1.115:53/UDP,10.10.1.115:9153/TCP,10.10.1.141:53/TCP,10.10.1.141:53/UDP,10.10.1.141:9153/TCP" k8sNamespace=kube-system k8sSvcName=kube-dns old-service=nil service="frontend:10.11.0.10/ports=[dns-tcp metrics dns]/selector=map[k8s-app:kube-dns]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[{0 kind-worker {10.10.1.115 {UDP 53} 0}} {0 kind-worker {10.10.1.141 {UDP 53} 0}}]" serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[{0 kind-worker {10.10.1.115 {UDP 53} 0}} {0 kind-worker {10.10.1.141 {UDP 53} 0}}]" serviceID=2 serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=2 subsys=service
level=debug msg="Adding new backend" backendID=4 backends="[{0 kind-worker {10.10.1.115 {UDP 53} 0}} {0 kind-worker {10.10.1.141 {UDP 53} 0}}]" l3n4Addr="{10.10.1.115 {UDP 53} 0}" serviceID=2 serviceIP="{10.11.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserting service entry" slaveSlot=1 subsys=map-lb svcKey="10.11.0.10:53" svcVal="4 (2) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=2 subsys=map-lb svcKey="10.11.0.10:53" svcVal="2 (2) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.10:53" svcVal="0 (2) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserting service" backends="[{0 kind-worker {10.10.1.115 {TCP 9153} 0}} {0 kind-worker {10.10.1.141 {TCP 9153} 0}}]" serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[{0 kind-worker {10.10.1.115 {TCP 9153} 0}} {0 kind-worker {10.10.1.141 {TCP 9153} 0}}]" serviceID=1 serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=1 subsys=service
level=debug msg="Adding new backend" backendID=5 backends="[{0 kind-worker {10.10.1.115 {TCP 9153} 0}} {0 kind-worker {10.10.1.141 {TCP 9153} 0}}]" l3n4Addr="{10.10.1.115 {TCP 9153} 0}" serviceID=1 serviceIP="{10.11.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcHealthCheckNodePort=0 svcTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserting service entry" slaveSlot=1 subsys=map-lb svcKey="10.11.0.10:9153" svcVal="5 (1) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=2 subsys=map-lb svcKey="10.11.0.10:9153" svcVal="3 (1) [FLAGS: 0x0]"
level=debug msg="Upserting service entry" slaveSlot=0 subsys=map-lb svcKey="10.11.0.10:9153" svcVal="0 (1) [FLAGS: 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=1 subsys=service
level=debug msg="Controller func execution time: 77.005µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Upserting IP into ipcache layer" identity="{39246 custom-resource false}" ipAddr=10.10.1.141 k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-f54bp key=0 namedPorts="map[dns:{53 17} dns-tcp:{53 6} metrics:{9153 6}]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=39246 ipAddr="{10.10.1.141 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 11.162667ms" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Upserting IP into ipcache layer" identity="{39246 custom-resource false}" ipAddr=10.10.1.115 k8sNamespace=kube-system k8sPodName=coredns-66bff467f8-8mx7s key=0 namedPorts="map[dns:{53 17} dns-tcp:{53 6} metrics:{9153 6}]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=39246 ipAddr="{10.10.1.115 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 9.554767ms" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 2.398µs" name=k8s-heartbeat subsys=controller uuid=c88e3e32-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 105.425µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Received node update event from custom-resource: types.Node{Name:\"kind-worker2\", Cluster:\"default\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x12, 0x0, 0x4}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xa, 0x2, 0xf9}}}, IPv4AllocCIDR:(*cidr.CIDR)(0xc00000e960), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xa, 0x2, 0xa3}, IPv6HealthIP:net.IP(nil), ClusterID:0, Source:\"custom-resource\", EncryptionKey:0x0, Labels:map[string]string(nil)}" subsys=nodemanager
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource false}" ipAddr=172.18.0.4 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=remote-node ipAddr="{172.18.0.4 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource false}" ipAddr=10.10.2.249 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=remote-node ipAddr="{10.10.2.249 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{health custom-resource false}" ipAddr=10.10.2.163 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=health ipAddr="{10.10.2.163 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg=insertNeighbor HardwareAddr="02:42:ac:12:00:04" Interface=eth0 LinkIndex=60 ipAddr=172.18.0.4 reason="insertNeighbor NeighSet" subsys=linux-datapath
level=debug msg="Updating tunnel map entry" allocCIDR=10.10.2.0/24 ipAddr=172.18.0.4 subsys=linux-datapath
level=debug msg="Updating tunnel map entry" endpoint=172.18.0.4 key=0 prefix=10.10.2.0 subsys=map-tunnel
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 56.99µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 94.2µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 67.63µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 125.903µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 118.035µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 113.415µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 149.786µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 123.598µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 202.624µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 199.556µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 181.64µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 211.85µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 220.555µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 263.214µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 114.673µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 224.123µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 160.247µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 141.123µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 242.905µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 206.092µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Performing regular background work" subsys=nodemanager syncInterval=1m23.177661667s
level=debug msg="Controller func execution time: 6.178314ms" name=k8s-heartbeat subsys=controller uuid=c88e3e32-c8e9-11ea-9724-8aaf378a454f
level=debug msg=insertNeighbor HardwareAddr="02:42:ac:12:00:03" Interface=eth0 LinkIndex=60 ipAddr=172.18.0.3 reason="insertNeighbor NeighSet" subsys=linux-datapath
level=debug msg="Updating tunnel map entry" allocCIDR=10.10.0.0/24 ipAddr=172.18.0.3 subsys=linux-datapath
level=debug msg="Updating tunnel map entry" endpoint=172.18.0.3 key=0 prefix=10.10.0.0 subsys=map-tunnel
level=debug msg="Controller func execution time: 226.904µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg=insertNeighbor HardwareAddr="02:42:ac:12:00:04" Interface=eth0 LinkIndex=60 ipAddr=172.18.0.4 reason="insertNeighbor NeighSet" subsys=linux-datapath
level=debug msg="Updating tunnel map entry" allocCIDR=10.10.2.0/24 ipAddr=172.18.0.4 subsys=linux-datapath
level=debug msg="Updating tunnel map entry" endpoint=172.18.0.4 key=0 prefix=10.10.2.0 subsys=map-tunnel
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 139.9µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 621.951µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 76.207µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 173.975µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 210.081µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 198.875µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 162.608µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 34.339µs" name=dns-garbage-collector-job subsys=controller uuid=cef0e57d-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=172.18.0.2 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{172.18.0.2 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{host local false}" ipAddr=10.10.1.228 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=host ipAddr="{10.10.1.228 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{world local false}" ipAddr=0.0.0.0/0 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity=world ipAddr="{0.0.0.0 00000000}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 2.054766ms" name=sync-endpoints-and-host-ips subsys=controller uuid=cef0da18-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 99.001µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 195.472µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 537.244µs" name=cilium-health-ep subsys=controller uuid=cef6939a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 58.296µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 164.504µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 118.495µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 260.355µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.1.228 nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.2 nodeName=kind-worker primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.1.241 nodeName=kind-worker primary=false protocol=http subsys=health-server
level=debug msg="Sending request for /cluster/nodes ..." subsys=health-server
level=debug msg="Greeting host" host="http://172.18.0.2:4240" ipAddr=172.18.0.2 nodeName=kind-worker path="Via L3" subsys=health-server
level=debug msg="Got cilium /cluster/nodes" subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.0.84 nodeName=kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Greeting successful" host="http://172.18.0.2:4240" ipAddr=172.18.0.2 nodeName=kind-worker path="Via L3" rtt="766.834µs" subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.0.179 nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.3 nodeName=kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.2.163 nodeName=kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.2.249 nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.4 nodeName=kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.3 nodeName=kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.0.84 nodeName=kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.0.179 nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.0.84 nodeName=kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.0.179 nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.3 nodeName=kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.4 nodeName=kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.2.163 nodeName=kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.2.249 nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.4 nodeName=kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.2.163 nodeName=kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.2.249 nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.18.0.4 nodeName=kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.10.2.163 nodeName=kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.10.2.249 nodeName=kind-worker2 primary=false subsys=health-server
level=debug msg="Greeting host" host="http://10.10.1.241:4240" ipAddr=10.10.1.241 nodeName=kind-worker path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://10.10.1.241:4240" ipAddr=10.10.1.241 nodeName=kind-worker path="Via L3" rtt="698.334µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=172.18.0.4 nodeName=kind-worker2 rtt=1.749455ms subsys=health-server
level=debug msg="Probe successful" ipAddr=10.10.0.84 nodeName=kind-control-plane rtt=1.839348ms subsys=health-server
level=debug msg="Probe successful" ipAddr=172.18.0.2 nodeName=kind-worker rtt=1.962988ms subsys=health-server
level=debug msg="Probe successful" ipAddr=10.10.1.241 nodeName=kind-worker rtt=2.005681ms subsys=health-server
level=debug msg="Probe successful" ipAddr=172.18.0.3 nodeName=kind-control-plane rtt=2.076202ms subsys=health-server
level=debug msg="Probe successful" ipAddr=10.10.2.163 nodeName=kind-worker2 rtt=2.13662ms subsys=health-server
level=debug msg="Controller func execution time: 138.251µs" name=sync-policymap-1909 subsys=controller uuid=d11265e5-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 126.97µs" name=sync-policymap-3097 subsys=controller uuid=d22d5161-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 113.385µs" name=sync-policymap-3398 subsys=controller uuid=d15fcc12-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 190.11µs" name=sync-policymap-243 subsys=controller uuid=d10f12ae-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 155.1µs" name=sync-policymap-945 subsys=controller uuid=d23a276f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 156.546µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 124.505µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 193.768µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 98.479µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 290.685µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 161.273µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 204.055µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Controller func execution time: 5.732422ms" name=k8s-heartbeat subsys=controller uuid=c88e3e32-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 151.762µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3398)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3398 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 156.232µs" name="sync-to-k8s-ciliumendpoint (3398)" subsys=controller uuid=cef27b3e-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (1909)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=1909 identity=26160 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 292.554µs" name="sync-to-k8s-ciliumendpoint (1909)" subsys=controller uuid=cf23b016-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (243)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=243 identity=4 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 47.464µs" name="sync-to-k8s-ciliumendpoint (243)" subsys=controller uuid=cf9a9b64-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Controller func execution time: 255.139µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=c8944f9a-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (3097)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=3097 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 172.856µs" name="sync-to-k8s-ciliumendpoint (3097)" subsys=controller uuid=d1c3221f-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Skipping CiliumEndpoint update because it has not changed" containerID= controller="sync-to-k8s-ciliumendpoint (945)" datapathPolicyRevision=2 desiredPolicyRevision=2 endpointID=945 identity=39246 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 196.509µs" name="sync-to-k8s-ciliumendpoint (945)" subsys=controller uuid=d1cd7724-c8e9-11ea-9724-8aaf378a454f
level=debug msg="Handling request for /healthz" subsys=health-server
